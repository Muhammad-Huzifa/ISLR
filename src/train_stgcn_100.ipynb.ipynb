{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13155294,"sourceType":"datasetVersion","datasetId":8335217},{"sourceId":13527533,"sourceType":"datasetVersion","datasetId":8589371},{"sourceId":621868,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":467731,"modelId":483555}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nFull training script for NSLT-100 using ST-GCN + Temporal Self-Attention.\n- Expects:\n  - LANDMARK_FILE: .npz of landmarks (keys are video ids)\n  - SPLIT_FILE: nslt_100.json (same format as your nslt_300.json)\n- Produces:\n  - best_stgcn_100.pth\n  - training_log.csv\n  - training_curves.png\n\nNotes / fixes compared to your earlier script:\n- Builds a label->index mapping from the train split so labels become contiguous 0..(num_classes-1)\n- Uses CosineAnnealingLR correctly (scheduler.step() each epoch)\n- Computes class weights from mapped labels\n- Computes top-1 and top-5 accuracy\n- Safer handling of landmark array shapes\n- Slightly reduced num_workers default for compat\n\nRun on Kaggle / local with GPU. Adjust paths and hyperparams at the top.\n\"\"\"\n\nimport os\nimport json\nimport math\nimport random\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# --------------------------\n# Config\n# --------------------------\nLANDMARK_FILE = '/kaggle/input/new-bones-joints/Landmarks_GCN_augmented.npz'\nSPLIT_FILE = '/kaggle/input/json-files/nslt_100.json'\nNUM_CLASSES = 100\nTOTAL_LANDMARKS = 67  # 15 pose + 2 mid + 42 hands + 8 face = 67\nC = 4                # x,y,z + (optionally) confidence or reserved channel\nT_FRAMES = 128\nBATCH_SIZE = 32\nEPOCHS = 200\nBASE_LR = 3e-4\nWEIGHT_DECAY = 1e-4\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nUSE_AMP = True\nMIXUP_ALPHA = 0.4\nLABEL_SMOOTHING = 0.1\nNUM_WORKERS = 2\n\n# reproducibility (not strictly deterministic)\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\n# --------------------------\n# Helpers (temporal resample + preprocessor)\n# --------------------------\n\ndef temporal_resample(sequence, target_len):\n    orig_len = sequence.shape[0]\n    if orig_len == target_len:\n        return sequence\n    if orig_len == 1:\n        return np.repeat(sequence, target_len, axis=0)\n    target_positions = np.linspace(0, orig_len - 1, target_len)\n    old_idx = np.arange(orig_len)\n    out = np.zeros((target_len, sequence.shape[1], sequence.shape[2]), dtype=sequence.dtype)\n    for v in range(sequence.shape[1]):\n        for c in range(sequence.shape[2]):\n            values = sequence[:, v, c]\n            out[:, v, c] = np.interp(target_positions, old_idx, values)\n    return out\n\n\nclass LandmarkPreprocessor:\n    def __init__(self, augment=True, target_frames=T_FRAMES):\n        self.augment = augment\n        self.target_frames = target_frames\n        # indices\n        self.POSE_IDXS = list(range(0, 17))  # includes 2 mid points assumption\n        self.LEFT_HAND_IDXS = list(range(17, 38))\n        self.RIGHT_HAND_IDXS = list(range(38, 59))\n        self.FACE_IDXS = list(range(59, 67))\n\n    def normalize_frame(self, frame):\n        normalized = frame.copy()\n        # Pose + face\n        try:\n            pose = frame[self.POSE_IDXS, :2]\n            if np.sum(np.abs(pose)) > 0:\n                nose = pose[0]\n                l_s = pose[1] if len(pose) > 1 else nose\n                r_s = pose[2] if len(pose) > 2 else nose + 0.1\n                if np.all(l_s == 0) or np.all(r_s == 0):\n                    l_s = np.nan_to_num(np.mean(pose, axis=0))\n                    r_s = l_s + 0.1\n                dist = np.linalg.norm(l_s - r_s)\n                if dist < 1e-4:\n                    dist = 0.1\n                hu = dist / 2.0\n                bw = 6 * hu\n                bh = 7 * hu\n                bx = nose[0] - 3 * hu\n                by = nose[1] - 0.5 * hu\n                pf_idxs = self.POSE_IDXS + self.FACE_IDXS\n                normalized[pf_idxs, 0] = (frame[pf_idxs, 0] - bx) / max(bw, 1e-6) - 0.5\n                normalized[pf_idxs, 1] = (frame[pf_idxs, 1] - by) / max(bh, 1e-6) - 0.5\n        except Exception:\n            pass\n\n        # hands\n        for h_idxs in [self.LEFT_HAND_IDXS, self.RIGHT_HAND_IDXS]:\n            try:\n                hand = frame[h_idxs]\n                if np.sum(np.abs(hand)) > 0:\n                    xs, ys = hand[:, 0], hand[:, 1]\n                    if (np.max(xs) != np.min(xs)) or (np.max(ys) != np.min(ys)):\n                        xmin, xmax, ymin, ymax = np.min(xs), np.max(xs), np.min(ys), np.max(ys)\n                        box_size = max(xmax - xmin, ymax - ymin, 1e-4)\n                        normalized[h_idxs, 0] = (hand[:, 0] - xmin) / box_size - 0.5\n                        normalized[h_idxs, 1] = (hand[:, 1] - ymin) / box_size - 0.5\n            except Exception:\n                continue\n\n        # z normalize by torso median\n        try:\n            torso_z = frame[self.POSE_IDXS, 2]\n            torso_z_nonzero = torso_z[torso_z != 0]\n            if len(torso_z_nonzero) > 0:\n                median_z = np.median(torso_z_nonzero)\n                normalized[:, 2] = (frame[:, 2] - median_z)\n        except Exception:\n            pass\n\n        return normalized\n\n    def augment_sequence(self, seq):\n        # rotation noise\n        angle = random.uniform(-12, 12)\n        rad = math.radians(angle)\n        cos, sin = math.cos(rad), math.sin(rad)\n        R = np.array([[cos, -sin], [sin, cos]], dtype=np.float32)\n        xy = seq[:, :, :2].reshape(-1, 2)\n        xy = xy @ R.T\n        seq[:, :, :2] = xy.reshape(seq.shape[0], seq.shape[1], 2)\n        seq += np.random.normal(0, 0.003, size=seq.shape).astype(np.float32)\n\n        # temporal jitter\n        if random.random() < 0.5:\n            T = seq.shape[0]\n            jittered = []\n            for t in range(T):\n                if random.random() < 0.05 and t < T - 1:\n                    continue\n                jittered.append(seq[t])\n                if random.random() < 0.05:\n                    jittered.append(seq[t])\n            if len(jittered) < 2:\n                jittered = [seq[t] for t in range(T)]\n            jittered = np.stack(jittered, axis=0)\n            seq = temporal_resample(jittered, self.target_frames)\n        return seq\n\n    def __call__(self, landmark_sequence: np.ndarray, is_train: bool):\n        landmark_sequence = np.nan_to_num(landmark_sequence, 0.0).astype(np.float32)\n        seq = temporal_resample(landmark_sequence, self.target_frames)\n        seq = np.stack([self.normalize_frame(frame) for frame in seq], axis=0)\n        if is_train and self.augment:\n            seq = self.augment_sequence(seq)\n        return seq\n\n\n# --------------------------\n# Dataset\n# --------------------------\nclass WLASLLandmarkDataset(Dataset):\n    def __init__(self, landmark_path, split_file_path, split='train', preprocessor=None, label_map=None):\n        self.split = split\n        self.preprocessor = preprocessor if preprocessor is not None else LandmarkPreprocessor()\n        landmarks_data = dict(np.load(landmark_path, allow_pickle=True))\n        with open(split_file_path, 'r') as f:\n            split_data = json.load(f)\n        split_dict = split_data.get('root', split_data)\n        self.samples = []\n\n        for video_id, info in split_dict.items():\n            subset = info.get('subset') or info.get('split') or info.get('subset', None)\n            if subset == self.split:\n                if video_id in landmarks_data:\n                    action = info.get('action', [])\n                    if not isinstance(action, list) or len(action) == 0:\n                        continue\n                    raw_label = action[0]\n                    # map to contiguous label if mapping provided\n                    if label_map is None:\n                        label = int(raw_label)\n                    else:\n                        if str(raw_label) not in label_map:\n                            continue\n                        label = int(label_map[str(raw_label)])\n                    landmarks = landmarks_data[video_id]\n                    self.samples.append({\"video_id\": video_id, \"landmarks\": landmarks, \"label\": label})\n        print(f\"âœ… Created '{self.split}' split with {len(self.samples)} samples.\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n        landmark_sequence = sample[\"landmarks\"]\n        # handle flattened format\n        if isinstance(landmark_sequence, np.ndarray) and landmark_sequence.ndim == 2 and landmark_sequence.shape[1] == TOTAL_LANDMARKS * 3:\n            landmark_sequence = landmark_sequence.reshape(-1, TOTAL_LANDMARKS, 3)\n        processed = self.preprocessor(landmark_sequence, is_train=(self.split == 'train'))\n        processed = np.transpose(processed, (2, 0, 1)).astype(np.float32)  # (C, T, V)\n        label = int(sample['label'])\n        return torch.from_numpy(processed), torch.tensor(label, dtype=torch.long)\n\n\ndef collate_fn(batch):\n    xs, ys = zip(*batch)\n    xs = torch.stack(xs, dim=0)\n    ys = torch.tensor(ys, dtype=torch.long)\n    return xs, ys\n\n\n# --------------------------\n# Model (GraphConv, STGCNBlock, TemporalSelfAttention, STGCN)\n# --------------------------\nclass GraphConv(nn.Module):\n    def __init__(self, in_channels, out_channels, num_nodes, bias=True):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.theta = nn.Parameter(torch.Tensor(in_channels, out_channels))\n        self.A = nn.Parameter(torch.eye(num_nodes, dtype=torch.float32), requires_grad=True)\n        if bias:\n            self.bias = nn.Parameter(torch.zeros(out_channels))\n        else:\n            self.register_parameter('bias', None)\n        nn.init.xavier_uniform_(self.theta)\n\n    def forward(self, x):\n        B, C, T, V = x.shape\n        x_perm = x.permute(0, 2, 3, 1).contiguous().view(B * T, V, C)\n        y = x_perm @ self.theta\n        A = self.A.unsqueeze(0)\n        y = torch.bmm(A.repeat(B * T, 1, 1), y)\n        y = y.view(B, T, V, self.out_channels).permute(0, 3, 1, 2).contiguous()\n        if self.bias is not None:\n            y = y + self.bias.view(1, -1, 1, 1)\n        return y\n\n\nclass STGCNBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, num_nodes, kernel_size=9, stride=1, residual=True):\n        super().__init__()\n        self.gcn = GraphConv(in_channels, out_channels, num_nodes)\n        padding = (kernel_size - 1) // 2\n        self.tcn = nn.Sequential(\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=(kernel_size, 1), padding=(padding, 0), stride=(stride, 1)),\n            nn.BatchNorm2d(out_channels),\n            nn.Dropout(0.3)\n        )\n        if not residual:\n            self.residual = lambda x: 0\n        elif (in_channels == out_channels) and stride == 1:\n            self.residual = lambda x: x\n        else:\n            self.residual = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=(stride,1)),\n                nn.BatchNorm2d(out_channels),\n            )\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        res = self.residual(x)\n        x = self.gcn(x)\n        x = self.tcn(x)\n        x = x + res\n        return self.relu(x)\n\n\nclass TemporalSelfAttention(nn.Module):\n    def __init__(self, d_model, n_heads=4):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n        self.ln = nn.LayerNorm(d_model)\n\n    def forward(self, x):\n        B, C, T, V = x.shape\n        x = x.permute(0, 3, 2, 1).contiguous().view(B * V, T, C)\n        out, _ = self.attn(x, x, x)\n        out = self.ln(out + x)\n        out = out.view(B, V, T, C).permute(0, 3, 2, 1).contiguous()\n        return out\n\n\nclass STGCN(nn.Module):\n    def __init__(self, in_channels, num_class, num_nodes):\n        super().__init__()\n        self.data_bn = nn.BatchNorm1d(in_channels * num_nodes)\n        self.layers = nn.ModuleList([\n            STGCNBlock(in_channels, 64, num_nodes, kernel_size=9, stride=1, residual=False),\n            STGCNBlock(64, 64, num_nodes, kernel_size=9, stride=1),\n            STGCNBlock(64, 128, num_nodes, kernel_size=9, stride=2),\n            STGCNBlock(128, 256, num_nodes, kernel_size=9, stride=2),\n        ])\n        self.temporal_attn = TemporalSelfAttention(d_model=256, n_heads=4)\n        self.pool = nn.AdaptiveAvgPool2d((1,1))\n        self.fc = nn.Linear(256, num_class)\n\n    def forward(self, x):\n        B, C, T, V = x.shape\n        x = x.permute(0, 1, 3, 2).contiguous()\n        x = x.view(B, C * V, T)\n        x = self.data_bn(x)\n        x = x.view(B, C, V, T).permute(0, 1, 3, 2).contiguous()\n        for layer in self.layers:\n            x = layer(x)\n        x = self.temporal_attn(x)\n        x = self.pool(x)\n        x = x.view(B, -1)\n        out = self.fc(x)\n        return out\n\n\n# --------------------------\n# Metrics & Mixup\n# --------------------------\ndef mixup_data(x, y, alpha=1.0, device=None):\n    if alpha <= 0:\n        return x, y, y, 1.0\n    lam = np.random.beta(alpha, alpha)\n    batch_size = x.size(0)\n    index = torch.randperm(batch_size).to(device)\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\n\ndef accuracy_topk(output, target, topk=(1,5)):\n    maxk = max(topk)\n    batch_size = target.size(0)\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n    res = []\n    for k in topk:\n        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n        res.append((correct_k.mul_(100.0 / batch_size)).item())\n    return res  # list of percentages\n\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\n# --------------------------\n# Training pipeline\n# --------------------------\n\ndef prepare_label_map(split_file, target_num_classes=NUM_CLASSES):\n    with open(split_file, 'r') as f:\n        data = json.load(f)\n    split_dict = data.get('root', data)\n    train_actions = set()\n    for vid, info in split_dict.items():\n        subset = info.get('subset') or info.get('split') or info.get('subset', None)\n        if subset == 'train':\n            action = info.get('action', [])\n            if isinstance(action, list) and len(action) > 0:\n                train_actions.add(str(action[0]))\n    train_actions = sorted(list(train_actions), key=lambda x: int(x))\n    if len(train_actions) != target_num_classes:\n        print(f\"[Warning] Found {len(train_actions)} unique train actions, expected {target_num_classes}.\")\n    # build mapping\n    label_map = {raw_label: idx for idx, raw_label in enumerate(train_actions)}\n    return label_map\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T07:41:43.108129Z","iopub.execute_input":"2025-10-28T07:41:43.108386Z","iopub.status.idle":"2025-10-28T07:41:43.150795Z","shell.execute_reply.started":"2025-10-28T07:41:43.108369Z","shell.execute_reply":"2025-10-28T07:41:43.150226Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"\n\ndef main():\n    # build label_map from train split\n    label_map = prepare_label_map(SPLIT_FILE, target_num_classes=NUM_CLASSES)\n\n    preprocessor = LandmarkPreprocessor(augment=True, target_frames=T_FRAMES)\n    train_dataset = WLASLLandmarkDataset(LANDMARK_FILE, SPLIT_FILE, split='train', preprocessor=preprocessor, label_map=label_map)\n    val_dataset = WLASLLandmarkDataset(LANDMARK_FILE, SPLIT_FILE, split='val', preprocessor=preprocessor, label_map=label_map)\n    test_dataset = WLASLLandmarkDataset(LANDMARK_FILE, SPLIT_FILE, split='test', preprocessor=preprocessor, label_map=label_map)\n\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_fn)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_fn)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_fn)\n\n    # class weights\n    train_labels = [int(s['label']) for s in train_dataset.samples]\n    if len(train_labels) == 0:\n        raise RuntimeError('No training samples found. Check your split & label_map')\n    unique_classes = np.unique(train_labels)\n    class_weights = compute_class_weight('balanced', classes=np.arange(len(unique_classes)), y=train_labels)\n    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(DEVICE)\n\n    # model\n    model = STGCN(in_channels=C, num_class=len(unique_classes), num_nodes=TOTAL_LANDMARKS).to(DEVICE)\n    print(f\"Model has {count_parameters(model)} trainable parameters.\")\n    optimizer = torch.optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n    scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=LABEL_SMOOTHING)\n\n    best_val = 0.0\n    train_history = {'epoch': [], 'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'val_top5': []}\n\n    for epoch in range(EPOCHS):\n        model.train()\n        running_loss = 0.0\n        running_acc = 0.0\n        total_samples = 0\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} Train\")\n        for x, y in pbar:\n            x = x.to(DEVICE)\n            y = y.to(DEVICE)\n            batch_size = x.size(0)\n            if MIXUP_ALPHA > 0:\n                mixed_x, y_a, y_b, lam = mixup_data(x, y, alpha=MIXUP_ALPHA, device=DEVICE)\n            else:\n                mixed_x, y_a, y_b, lam = x, y, y, 1.0\n\n            optimizer.zero_grad()\n            with torch.cuda.amp.autocast(enabled=USE_AMP):\n                logits = model(mixed_x)\n                loss = lam * criterion(logits, y_a) + (1 - lam) * criterion(logits, y_b)\n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            scaler.step(optimizer)\n            scaler.update()\n\n            # accuracy on hard targets (y)\n            acc1 = accuracy_topk(logits.detach().cpu(), y.detach().cpu(), topk=(1,))[0]\n            running_loss += loss.item() * batch_size\n            running_acc += acc1 * batch_size / 100.0\n            total_samples += batch_size\n            pbar.set_postfix(loss=running_loss / total_samples, acc=(running_acc / total_samples * 100.0))\n\n        avg_train_loss = running_loss / len(train_dataset)\n        avg_train_acc = running_acc / len(train_dataset) * 100.0\n\n        # validation\n        model.eval()\n        val_loss = 0.0\n        val_acc = 0.0\n        val_top5 = 0.0\n        with torch.no_grad():\n            for x, y in tqdm(val_loader, desc=\"Validation\"):\n                x = x.to(DEVICE); y = y.to(DEVICE)\n                with torch.cuda.amp.autocast(enabled=USE_AMP):\n                    logits = model(x)\n                    loss = criterion(logits, y)\n                val_loss += loss.item() * x.size(0)\n                acc1, acc5 = accuracy_topk(logits.detach().cpu(), y.detach().cpu(), topk=(1,5))\n                val_acc += acc1 * x.size(0) / 100.0\n                val_top5 += acc5 * x.size(0) / 100.0\n        avg_val_loss = val_loss / len(val_dataset) if len(val_dataset) > 0 else 0.0\n        avg_val_acc = val_acc / len(val_dataset) * 100.0 if len(val_dataset) > 0 else 0.0\n        avg_val_top5 = val_top5 / len(val_dataset) * 100.0 if len(val_dataset) > 0 else 0.0\n\n        scheduler.step()\n\n        print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss {avg_train_loss:.4f} Acc {avg_train_acc:.2f}% | Val Loss {avg_val_loss:.4f} Acc {avg_val_acc:.2f}% Top5 {avg_val_top5:.2f}%\")\n\n        train_history['epoch'].append(epoch+1)\n        train_history['train_loss'].append(avg_train_loss)\n        train_history['train_acc'].append(avg_train_acc)\n        train_history['val_loss'].append(avg_val_loss)\n        train_history['val_acc'].append(avg_val_acc)\n        train_history['val_top5'].append(avg_val_top5)\n\n        if avg_val_acc > best_val:\n            best_val = avg_val_acc\n            torch.save(model.state_dict(), f\"best_stgcn_{NUM_CLASSES}.pth\")\n            print(f\"âœ… New best model: {best_val:.2f}%\")\n\n    print(\"Training complete. Best val:\", best_val)\n\n    # save history\n    df = pd.DataFrame(train_history)\n    df.to_csv('training_log.csv', index=False)\n    print(\"Training log saved to training_log.csv\")\n\n    # curves\n    plt.figure(figsize=(12,5))\n    plt.subplot(1,2,1)\n    plt.plot(df['epoch'], df['train_loss'], label='Train Loss')\n    plt.plot(df['epoch'], df['val_loss'], label='Val Loss')\n    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.title('Loss Curve')\n    plt.subplot(1,2,2)\n    plt.plot(df['epoch'], df['train_acc'], label='Train Acc')\n    plt.plot(df['epoch'], df['val_acc'], label='Val Acc')\n    plt.xlabel('Epoch'); plt.ylabel('Accuracy (%)'); plt.legend(); plt.title('Accuracy Curve')\n    plt.savefig('training_curves.png')\n    plt.close()\n    print(\"Training curves saved to training_curves.png\")\n\n    # test\n    model.load_state_dict(torch.load(f\"best_stgcn_{NUM_CLASSES}.pth\", map_location=DEVICE))\n    model.eval()\n    test_acc = 0.0\n    test_top5 = 0.0\n    with torch.no_grad():\n        for x, y in tqdm(test_loader, desc=\"Testing\"):\n            x = x.to(DEVICE); y = y.to(DEVICE)\n            with torch.cuda.amp.autocast(enabled=USE_AMP):\n                logits = model(x)\n            acc1, acc5 = accuracy_topk(logits.detach().cpu(), y.detach().cpu(), topk=(1,5))\n            test_acc += acc1 * x.size(0) / 100.0\n            test_top5 += acc5 * x.size(0) / 100.0\n    avg_test_acc = test_acc / len(test_dataset) * 100.0 if len(test_dataset) > 0 else 0.0\n    avg_test_top5 = test_top5 / len(test_dataset) * 100.0 if len(test_dataset) > 0 else 0.0\n    print(f\"Final Test Accuracy: {avg_test_acc:.2f}% Top5: {avg_test_top5:.2f}%\")\n\n\nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T07:41:46.323224Z","iopub.execute_input":"2025-10-28T07:41:46.323945Z","iopub.status.idle":"2025-10-28T09:14:37.757562Z","shell.execute_reply.started":"2025-10-28T07:41:46.323918Z","shell.execute_reply":"2025-10-28T09:14:37.756586Z"}},"outputs":[{"name":"stdout","text":"âœ… Created 'train' split with 1442 samples.\nâœ… Created 'val' split with 338 samples.\nâœ… Created 'test' split with 258 samples.\nModel has 1209376 trainable parameters.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.02it/s, acc=0.832, loss=4.62]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/200 | Train Loss 4.6159 Acc 0.83% | Val Loss 4.4655 Acc 2.66% Top5 12.13%\nâœ… New best model: 2.66%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.98it/s, acc=2.43, loss=4.38]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/200 | Train Loss 4.3758 Acc 2.43% | Val Loss 4.2024 Acc 5.33% Top5 18.34%\nâœ… New best model: 5.33%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.00it/s, acc=4.51, loss=4.25]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/200 | Train Loss 4.2510 Acc 4.51% | Val Loss 4.1183 Acc 7.40% Top5 22.49%\nâœ… New best model: 7.40%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.07it/s, acc=4.23, loss=4.07]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/200 | Train Loss 4.0749 Acc 4.23% | Val Loss 3.9360 Acc 9.47% Top5 30.77%\nâœ… New best model: 9.47%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.97it/s, acc=6.66, loss=3.99]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/200 | Train Loss 3.9904 Acc 6.66% | Val Loss 3.8397 Acc 9.76% Top5 35.80%\nâœ… New best model: 9.76%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.04it/s, acc=7.21, loss=3.96]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/200 | Train Loss 3.9587 Acc 7.21% | Val Loss 3.8081 Acc 13.91% Top5 36.69%\nâœ… New best model: 13.91%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.05it/s, acc=5.89, loss=3.81]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/200 | Train Loss 3.8121 Acc 5.89% | Val Loss 3.7181 Acc 16.27% Top5 42.60%\nâœ… New best model: 16.27%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.04it/s, acc=7.35, loss=3.81]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/200 | Train Loss 3.8071 Acc 7.35% | Val Loss 3.7156 Acc 15.38% Top5 42.60%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.05it/s, acc=12.4, loss=3.62]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/200 | Train Loss 3.6167 Acc 12.41% | Val Loss 3.5291 Acc 20.71% Top5 48.82%\nâœ… New best model: 20.71%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.98it/s, acc=13.7, loss=3.61]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/200 | Train Loss 3.6091 Acc 13.66% | Val Loss 3.4699 Acc 23.37% Top5 54.14%\nâœ… New best model: 23.37%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.02it/s, acc=14.5, loss=3.47]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/200 | Train Loss 3.4720 Acc 14.49% | Val Loss 3.4580 Acc 25.74% Top5 53.55%\nâœ… New best model: 25.74%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.05it/s, acc=16.8, loss=3.39]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/200 | Train Loss 3.3922 Acc 16.78% | Val Loss 3.3368 Acc 29.88% Top5 55.62%\nâœ… New best model: 29.88%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.02it/s, acc=17.6, loss=3.4] \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:05<00:00,  2.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/200 | Train Loss 3.3969 Acc 17.61% | Val Loss 3.3319 Acc 23.96% Top5 57.69%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.96it/s, acc=17.1, loss=3.35]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/200 | Train Loss 3.3470 Acc 17.06% | Val Loss 3.2094 Acc 30.47% Top5 59.47%\nâœ… New best model: 30.47%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  2.00it/s, acc=22.4, loss=3.18]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/200 | Train Loss 3.1810 Acc 22.40% | Val Loss 3.1832 Acc 29.29% Top5 65.38%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.01it/s, acc=21.9, loss=3.06]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16/200 | Train Loss 3.0603 Acc 21.91% | Val Loss 2.9824 Acc 37.87% Top5 68.05%\nâœ… New best model: 37.87%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.06it/s, acc=21.5, loss=3.02]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:05<00:00,  2.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17/200 | Train Loss 3.0223 Acc 21.50% | Val Loss 2.9670 Acc 35.21% Top5 68.64%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.06it/s, acc=25.1, loss=2.96]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18/200 | Train Loss 2.9609 Acc 25.10% | Val Loss 2.8612 Acc 40.24% Top5 71.89%\nâœ… New best model: 40.24%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.99it/s, acc=23.1, loss=2.84]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19/200 | Train Loss 2.8391 Acc 23.09% | Val Loss 2.7791 Acc 41.12% Top5 74.56%\nâœ… New best model: 41.12%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.02it/s, acc=25.2, loss=2.74]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20/200 | Train Loss 2.7423 Acc 25.24% | Val Loss 2.6919 Acc 44.67% Top5 75.15%\nâœ… New best model: 44.67%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.08it/s, acc=30.5, loss=2.72]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21/200 | Train Loss 2.7182 Acc 30.51% | Val Loss 2.6092 Acc 45.86% Top5 77.22%\nâœ… New best model: 45.86%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.02it/s, acc=34.5, loss=2.52]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22/200 | Train Loss 2.5243 Acc 34.54% | Val Loss 2.4898 Acc 50.89% Top5 78.70%\nâœ… New best model: 50.89%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.97it/s, acc=37.7, loss=2.6] \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23/200 | Train Loss 2.5984 Acc 37.73% | Val Loss 2.4680 Acc 51.78% Top5 80.77%\nâœ… New best model: 51.78%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.04it/s, acc=28.8, loss=2.42]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24/200 | Train Loss 2.4241 Acc 28.78% | Val Loss 2.4034 Acc 53.55% Top5 79.88%\nâœ… New best model: 53.55%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.08it/s, acc=35.8, loss=2.36]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25/200 | Train Loss 2.3586 Acc 35.78% | Val Loss 2.4108 Acc 51.48% Top5 81.95%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.01it/s, acc=46.5, loss=2.24]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26/200 | Train Loss 2.2388 Acc 46.46% | Val Loss 2.3309 Acc 51.18% Top5 83.73%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.04it/s, acc=45, loss=2.2]   \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27/200 | Train Loss 2.2029 Acc 45.01% | Val Loss 2.2633 Acc 58.58% Top5 83.73%\nâœ… New best model: 58.58%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.04it/s, acc=39, loss=2.29]  \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28/200 | Train Loss 2.2946 Acc 39.04% | Val Loss 2.3283 Acc 52.66% Top5 83.43%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.05it/s, acc=43.3, loss=2.18]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29/200 | Train Loss 2.1756 Acc 43.27% | Val Loss 2.2779 Acc 57.10% Top5 84.62%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.98it/s, acc=39.9, loss=2.32]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:05<00:00,  2.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30/200 | Train Loss 2.3207 Acc 39.94% | Val Loss 2.2914 Acc 54.73% Top5 84.91%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=43.8, loss=2.19]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31/200 | Train Loss 2.1889 Acc 43.83% | Val Loss 2.2432 Acc 57.69% Top5 84.62%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.06it/s, acc=42.1, loss=2.12]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32/200 | Train Loss 2.1229 Acc 42.09% | Val Loss 2.2714 Acc 56.21% Top5 84.91%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.04it/s, acc=28.8, loss=2.16]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33/200 | Train Loss 2.1570 Acc 28.78% | Val Loss 2.1294 Acc 62.13% Top5 87.87%\nâœ… New best model: 62.13%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=57.8, loss=2.02]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34/200 | Train Loss 2.0234 Acc 57.84% | Val Loss 2.1799 Acc 60.65% Top5 86.09%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=35.2, loss=2.12]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35/200 | Train Loss 2.1240 Acc 35.16% | Val Loss 2.1918 Acc 57.99% Top5 86.09%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=57.6, loss=1.97]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36/200 | Train Loss 1.9654 Acc 57.56% | Val Loss 2.1675 Acc 59.76% Top5 86.39%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.04it/s, acc=44.5, loss=1.95]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37/200 | Train Loss 1.9484 Acc 44.52% | Val Loss 2.0539 Acc 64.20% Top5 87.87%\nâœ… New best model: 64.20%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.99it/s, acc=54.2, loss=2.07]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38/200 | Train Loss 2.0678 Acc 54.23% | Val Loss 2.1009 Acc 60.65% Top5 88.17%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=42.2, loss=1.99]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39/200 | Train Loss 1.9936 Acc 42.16% | Val Loss 2.0777 Acc 64.20% Top5 89.05%\nâœ… New best model: 64.20%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=50, loss=2.06]  \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 40/200 | Train Loss 2.0591 Acc 50.00% | Val Loss 2.0282 Acc 65.98% Top5 88.17%\nâœ… New best model: 65.98%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.01it/s, acc=35.6, loss=2.03]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 41/200 | Train Loss 2.0258 Acc 35.64% | Val Loss 2.0771 Acc 62.72% Top5 87.28%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.02it/s, acc=39.5, loss=2.21]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:05<00:00,  2.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 42/200 | Train Loss 2.2073 Acc 39.53% | Val Loss 1.9980 Acc 65.68% Top5 88.46%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.04it/s, acc=49.9, loss=1.87]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 43/200 | Train Loss 1.8689 Acc 49.86% | Val Loss 2.0014 Acc 64.79% Top5 89.64%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.98it/s, acc=50.7, loss=1.87]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 44/200 | Train Loss 1.8729 Acc 50.69% | Val Loss 2.0069 Acc 66.57% Top5 87.57%\nâœ… New best model: 66.57%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:24<00:00,  1.91it/s, acc=44.1, loss=1.76]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:05<00:00,  2.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 45/200 | Train Loss 1.7570 Acc 44.11% | Val Loss 2.0132 Acc 66.57% Top5 89.94%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.99it/s, acc=43.8, loss=1.85]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 46/200 | Train Loss 1.8509 Acc 43.83% | Val Loss 2.0101 Acc 65.98% Top5 88.46%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=53.7, loss=1.91]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 47/200 | Train Loss 1.9140 Acc 53.74% | Val Loss 1.9189 Acc 68.93% Top5 90.53%\nâœ… New best model: 68.93%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=48.7, loss=1.78]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 48/200 | Train Loss 1.7811 Acc 48.68% | Val Loss 2.0562 Acc 62.43% Top5 88.46%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.04it/s, acc=44.7, loss=1.86]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 49/200 | Train Loss 1.8591 Acc 44.66% | Val Loss 1.9788 Acc 68.05% Top5 88.76%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.01it/s, acc=44.3, loss=1.86]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 50/200 | Train Loss 1.8632 Acc 44.31% | Val Loss 2.0537 Acc 64.79% Top5 86.39%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 51/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.98it/s, acc=50.8, loss=1.72]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 51/200 | Train Loss 1.7242 Acc 50.76% | Val Loss 2.0052 Acc 65.98% Top5 87.28%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 52/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=39.1, loss=2]   \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:05<00:00,  2.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 52/200 | Train Loss 1.9980 Acc 39.11% | Val Loss 2.0639 Acc 63.02% Top5 86.39%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 53/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=55.8, loss=1.72]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 53/200 | Train Loss 1.7236 Acc 55.83% | Val Loss 1.9438 Acc 68.34% Top5 88.76%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 54/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.99it/s, acc=49.9, loss=1.72]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 54/200 | Train Loss 1.7184 Acc 49.86% | Val Loss 2.0506 Acc 64.79% Top5 86.98%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 55/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=44.9, loss=1.83]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:05<00:00,  2.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 55/200 | Train Loss 1.8308 Acc 44.94% | Val Loss 1.9381 Acc 68.05% Top5 87.87%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 56/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.00it/s, acc=52.1, loss=1.68]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:05<00:00,  2.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 56/200 | Train Loss 1.6842 Acc 52.15% | Val Loss 1.9687 Acc 66.27% Top5 87.87%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 57/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.02it/s, acc=34, loss=1.75]  \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:05<00:00,  2.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 57/200 | Train Loss 1.7480 Acc 34.05% | Val Loss 1.8669 Acc 70.71% Top5 91.42%\nâœ… New best model: 70.71%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 58/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.95it/s, acc=51.6, loss=1.78]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 58/200 | Train Loss 1.7845 Acc 51.60% | Val Loss 2.0033 Acc 68.34% Top5 87.28%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 59/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.00it/s, acc=37.7, loss=1.89]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 59/200 | Train Loss 1.8854 Acc 37.73% | Val Loss 1.9310 Acc 66.86% Top5 89.35%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 60/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.99it/s, acc=64.4, loss=1.48]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 60/200 | Train Loss 1.4819 Acc 64.36% | Val Loss 1.9564 Acc 67.75% Top5 89.64%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 61/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  2.00it/s, acc=47.6, loss=1.77]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 61/200 | Train Loss 1.7706 Acc 47.64% | Val Loss 1.9293 Acc 68.93% Top5 90.53%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 62/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.94it/s, acc=53.7, loss=1.8] \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:05<00:00,  1.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 62/200 | Train Loss 1.8026 Acc 53.68% | Val Loss 1.9681 Acc 69.53% Top5 89.05%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 63/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:25<00:00,  1.82it/s, acc=58.7, loss=1.68]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 63/200 | Train Loss 1.6788 Acc 58.74% | Val Loss 1.8421 Acc 72.49% Top5 90.83%\nâœ… New best model: 72.49%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 64/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.06it/s, acc=59.6, loss=1.77]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 64/200 | Train Loss 1.7679 Acc 59.57% | Val Loss 1.9343 Acc 69.23% Top5 88.76%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 65/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.05it/s, acc=52.6, loss=1.82]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 65/200 | Train Loss 1.8195 Acc 52.64% | Val Loss 1.9351 Acc 68.05% Top5 88.46%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 66/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.07it/s, acc=50.2, loss=1.72]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 66/200 | Train Loss 1.7165 Acc 50.21% | Val Loss 1.9090 Acc 71.01% Top5 88.76%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 67/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.01it/s, acc=49.4, loss=1.69]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 67/200 | Train Loss 1.6912 Acc 49.38% | Val Loss 1.9432 Acc 69.82% Top5 89.35%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 68/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.99it/s, acc=40.9, loss=1.66]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 68/200 | Train Loss 1.6629 Acc 40.92% | Val Loss 1.9215 Acc 69.53% Top5 88.46%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 69/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.04it/s, acc=55.3, loss=1.91]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 69/200 | Train Loss 1.9090 Acc 55.27% | Val Loss 1.9255 Acc 69.82% Top5 88.76%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 70/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.05it/s, acc=41.7, loss=1.85]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 70/200 | Train Loss 1.8513 Acc 41.75% | Val Loss 1.9122 Acc 68.93% Top5 88.46%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 71/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=52.7, loss=1.89]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 71/200 | Train Loss 1.8891 Acc 52.70% | Val Loss 1.9069 Acc 70.12% Top5 89.64%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 72/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=37.9, loss=1.51]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 72/200 | Train Loss 1.5080 Acc 37.93% | Val Loss 1.8600 Acc 72.78% Top5 89.94%\nâœ… New best model: 72.78%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 73/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=39.9, loss=1.68]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 73/200 | Train Loss 1.6779 Acc 39.94% | Val Loss 1.8993 Acc 71.01% Top5 88.76%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 74/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.02it/s, acc=57.8, loss=1.68]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 74/200 | Train Loss 1.6775 Acc 57.84% | Val Loss 1.8906 Acc 69.23% Top5 88.76%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 75/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.04it/s, acc=56.4, loss=1.83]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 75/200 | Train Loss 1.8290 Acc 56.45% | Val Loss 1.8753 Acc 71.60% Top5 89.35%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 76/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.02it/s, acc=60.8, loss=1.6] \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 76/200 | Train Loss 1.6031 Acc 60.82% | Val Loss 1.8397 Acc 73.67% Top5 88.17%\nâœ… New best model: 73.67%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 77/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.04it/s, acc=39, loss=1.63]  \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 77/200 | Train Loss 1.6295 Acc 39.04% | Val Loss 1.9246 Acc 70.71% Top5 87.57%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 78/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.05it/s, acc=54.2, loss=1.79]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 78/200 | Train Loss 1.7936 Acc 54.16% | Val Loss 1.9273 Acc 68.05% Top5 89.64%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 79/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.04it/s, acc=46.9, loss=1.61]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 79/200 | Train Loss 1.6054 Acc 46.88% | Val Loss 1.8691 Acc 71.89% Top5 90.53%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 80/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.95it/s, acc=50.4, loss=1.72]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 80/200 | Train Loss 1.7178 Acc 50.42% | Val Loss 1.8509 Acc 72.19% Top5 90.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 81/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.07it/s, acc=47.9, loss=1.78]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 81/200 | Train Loss 1.7760 Acc 47.85% | Val Loss 1.8716 Acc 70.71% Top5 88.46%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 82/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=54, loss=1.73]  \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 82/200 | Train Loss 1.7270 Acc 54.02% | Val Loss 1.8743 Acc 71.01% Top5 88.76%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 83/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.04it/s, acc=47.7, loss=1.63]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 83/200 | Train Loss 1.6283 Acc 47.71% | Val Loss 1.8334 Acc 72.78% Top5 90.24%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 84/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=41.3, loss=1.86]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 84/200 | Train Loss 1.8647 Acc 41.33% | Val Loss 1.8886 Acc 72.19% Top5 89.05%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 85/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.04it/s, acc=43.6, loss=1.81]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 85/200 | Train Loss 1.8088 Acc 43.62% | Val Loss 1.9172 Acc 71.89% Top5 90.53%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 86/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.97it/s, acc=51.6, loss=1.76]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 86/200 | Train Loss 1.7622 Acc 51.60% | Val Loss 1.8685 Acc 72.19% Top5 90.24%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 87/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.01it/s, acc=46.8, loss=1.76]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 87/200 | Train Loss 1.7633 Acc 46.81% | Val Loss 1.8527 Acc 73.37% Top5 90.24%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 88/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.06it/s, acc=54.9, loss=1.64]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 88/200 | Train Loss 1.6425 Acc 54.85% | Val Loss 1.8732 Acc 72.19% Top5 89.64%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 89/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.02it/s, acc=42.1, loss=1.82]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:05<00:00,  2.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 89/200 | Train Loss 1.8234 Acc 42.09% | Val Loss 1.8438 Acc 73.37% Top5 89.64%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 90/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.96it/s, acc=54.2, loss=1.71]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:05<00:00,  2.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 90/200 | Train Loss 1.7136 Acc 54.16% | Val Loss 1.8465 Acc 76.33% Top5 89.35%\nâœ… New best model: 76.33%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 91/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  2.00it/s, acc=38.6, loss=1.78]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 91/200 | Train Loss 1.7777 Acc 38.63% | Val Loss 1.8613 Acc 71.89% Top5 89.64%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 92/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.05it/s, acc=47.9, loss=1.66]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 92/200 | Train Loss 1.6613 Acc 47.92% | Val Loss 1.8423 Acc 71.60% Top5 89.35%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 93/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=49, loss=1.65]  \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 93/200 | Train Loss 1.6503 Acc 48.96% | Val Loss 1.8992 Acc 71.60% Top5 89.64%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 94/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.00it/s, acc=61.9, loss=1.6] \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 94/200 | Train Loss 1.6039 Acc 61.86% | Val Loss 1.8473 Acc 74.26% Top5 90.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 95/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.98it/s, acc=55.5, loss=1.91]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 95/200 | Train Loss 1.9073 Acc 55.55% | Val Loss 1.8633 Acc 70.41% Top5 88.76%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 96/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.01it/s, acc=62.3, loss=1.65]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 96/200 | Train Loss 1.6468 Acc 62.34% | Val Loss 1.8265 Acc 73.08% Top5 89.64%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 97/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  2.00it/s, acc=45.8, loss=1.72]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 97/200 | Train Loss 1.7188 Acc 45.84% | Val Loss 1.8067 Acc 72.78% Top5 90.24%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 98/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=55.4, loss=1.88]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:05<00:00,  2.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 98/200 | Train Loss 1.8779 Acc 55.41% | Val Loss 1.8124 Acc 71.60% Top5 90.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 99/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.99it/s, acc=43.6, loss=1.87]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 99/200 | Train Loss 1.8652 Acc 43.55% | Val Loss 1.8240 Acc 71.60% Top5 90.53%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 100/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.96it/s, acc=58.3, loss=1.61]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 100/200 | Train Loss 1.6071 Acc 58.32% | Val Loss 1.8257 Acc 72.19% Top5 90.24%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 101/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.01it/s, acc=59.8, loss=1.64]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 101/200 | Train Loss 1.6395 Acc 59.78% | Val Loss 1.8344 Acc 72.19% Top5 89.35%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 102/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.00it/s, acc=56.9, loss=1.59]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 102/200 | Train Loss 1.5852 Acc 56.93% | Val Loss 1.8279 Acc 71.89% Top5 90.53%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 103/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.01it/s, acc=49.7, loss=1.66]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 103/200 | Train Loss 1.6576 Acc 49.72% | Val Loss 1.8051 Acc 76.04% Top5 90.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 104/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.00it/s, acc=46.3, loss=1.59]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 104/200 | Train Loss 1.5889 Acc 46.32% | Val Loss 1.7982 Acc 76.04% Top5 89.94%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 105/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.02it/s, acc=45.6, loss=1.64]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 105/200 | Train Loss 1.6443 Acc 45.63% | Val Loss 1.8122 Acc 73.96% Top5 90.53%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 106/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.01it/s, acc=62, loss=1.51]  \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:05<00:00,  2.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 106/200 | Train Loss 1.5139 Acc 62.00% | Val Loss 1.8389 Acc 71.89% Top5 89.94%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 107/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.01it/s, acc=48.3, loss=1.63]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 107/200 | Train Loss 1.6300 Acc 48.27% | Val Loss 1.7981 Acc 75.44% Top5 90.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 108/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.01it/s, acc=52.1, loss=1.65]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 108/200 | Train Loss 1.6533 Acc 52.08% | Val Loss 1.8203 Acc 75.15% Top5 90.24%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 109/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=40.5, loss=1.72]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:05<00:00,  2.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 109/200 | Train Loss 1.7172 Acc 40.50% | Val Loss 1.8449 Acc 73.08% Top5 89.64%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 110/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  2.00it/s, acc=50.9, loss=1.65]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 110/200 | Train Loss 1.6534 Acc 50.90% | Val Loss 1.7968 Acc 76.04% Top5 89.64%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 111/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.06it/s, acc=53.1, loss=1.51]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 111/200 | Train Loss 1.5138 Acc 53.12% | Val Loss 1.7823 Acc 75.15% Top5 90.53%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 112/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.01it/s, acc=60.3, loss=1.62]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:05<00:00,  2.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 112/200 | Train Loss 1.6152 Acc 60.26% | Val Loss 1.8103 Acc 74.56% Top5 89.94%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 113/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.97it/s, acc=43.1, loss=1.67]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 113/200 | Train Loss 1.6682 Acc 43.13% | Val Loss 1.8199 Acc 73.67% Top5 90.24%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 114/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=48.5, loss=1.75]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 114/200 | Train Loss 1.7513 Acc 48.54% | Val Loss 1.7889 Acc 73.37% Top5 89.94%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 115/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.99it/s, acc=48.5, loss=1.52]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 115/200 | Train Loss 1.5235 Acc 48.47% | Val Loss 1.8133 Acc 74.56% Top5 90.24%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 116/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.05it/s, acc=64.4, loss=1.6] \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 116/200 | Train Loss 1.6048 Acc 64.42% | Val Loss 1.8072 Acc 75.74% Top5 89.94%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 117/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.01it/s, acc=52, loss=1.65]  \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 117/200 | Train Loss 1.6510 Acc 52.01% | Val Loss 1.7867 Acc 75.15% Top5 89.35%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 118/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.02it/s, acc=50.8, loss=1.82]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 118/200 | Train Loss 1.8225 Acc 50.76% | Val Loss 1.7849 Acc 74.56% Top5 90.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 119/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  2.00it/s, acc=42.8, loss=1.71]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 119/200 | Train Loss 1.7084 Acc 42.79% | Val Loss 1.7917 Acc 75.44% Top5 90.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 120/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  2.00it/s, acc=56.2, loss=1.85]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 120/200 | Train Loss 1.8507 Acc 56.17% | Val Loss 1.8004 Acc 75.44% Top5 90.53%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 121/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.98it/s, acc=47.8, loss=1.53]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 121/200 | Train Loss 1.5316 Acc 47.78% | Val Loss 1.7946 Acc 74.56% Top5 90.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 122/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.01it/s, acc=54.8, loss=1.56]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 122/200 | Train Loss 1.5572 Acc 54.79% | Val Loss 1.7763 Acc 72.49% Top5 91.72%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 123/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.96it/s, acc=40.2, loss=1.65]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 123/200 | Train Loss 1.6460 Acc 40.15% | Val Loss 1.7954 Acc 75.44% Top5 89.64%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 124/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.02it/s, acc=57.1, loss=1.66]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 124/200 | Train Loss 1.6573 Acc 57.07% | Val Loss 1.7924 Acc 75.44% Top5 89.64%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 125/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.04it/s, acc=35.2, loss=1.6] \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 125/200 | Train Loss 1.5962 Acc 35.23% | Val Loss 1.7902 Acc 73.67% Top5 90.53%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 126/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.00it/s, acc=43.5, loss=1.67]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 126/200 | Train Loss 1.6747 Acc 43.48% | Val Loss 1.8054 Acc 73.96% Top5 89.05%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 127/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.04it/s, acc=50.5, loss=1.82]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 127/200 | Train Loss 1.8248 Acc 50.49% | Val Loss 1.7937 Acc 73.08% Top5 90.24%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 128/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.01it/s, acc=58.3, loss=1.69]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 128/200 | Train Loss 1.6900 Acc 58.32% | Val Loss 1.7631 Acc 74.56% Top5 90.53%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 129/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.00it/s, acc=62.6, loss=1.5] \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 129/200 | Train Loss 1.5027 Acc 62.62% | Val Loss 1.7734 Acc 75.74% Top5 89.94%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 130/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.00it/s, acc=47.7, loss=1.65]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 130/200 | Train Loss 1.6537 Acc 47.71% | Val Loss 1.7879 Acc 74.26% Top5 89.94%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 131/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=55.5, loss=1.61]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 131/200 | Train Loss 1.6143 Acc 55.55% | Val Loss 1.7542 Acc 76.33% Top5 90.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 132/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.96it/s, acc=47.5, loss=1.69]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 132/200 | Train Loss 1.6860 Acc 47.50% | Val Loss 1.7629 Acc 76.33% Top5 90.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 133/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.99it/s, acc=50.3, loss=1.71]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 133/200 | Train Loss 1.7058 Acc 50.28% | Val Loss 1.8121 Acc 74.85% Top5 89.94%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 134/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.04it/s, acc=55.1, loss=1.54]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 134/200 | Train Loss 1.5416 Acc 55.06% | Val Loss 1.7666 Acc 74.85% Top5 91.72%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 135/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.02it/s, acc=50.8, loss=1.68]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 135/200 | Train Loss 1.6841 Acc 50.76% | Val Loss 1.7811 Acc 75.44% Top5 90.24%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 136/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.01it/s, acc=45, loss=1.7]   \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 136/200 | Train Loss 1.6960 Acc 45.01% | Val Loss 1.7603 Acc 74.56% Top5 91.42%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 137/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.04it/s, acc=55.1, loss=1.67]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 137/200 | Train Loss 1.6691 Acc 55.13% | Val Loss 1.7656 Acc 74.85% Top5 90.24%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 138/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.04it/s, acc=47.3, loss=1.66]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:05<00:00,  2.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 138/200 | Train Loss 1.6595 Acc 47.30% | Val Loss 1.7884 Acc 73.67% Top5 91.12%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 139/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.01it/s, acc=50.2, loss=1.74]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 139/200 | Train Loss 1.7410 Acc 50.21% | Val Loss 1.7573 Acc 76.33% Top5 90.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 140/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.98it/s, acc=39.9, loss=1.4] \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 140/200 | Train Loss 1.4043 Acc 39.88% | Val Loss 1.7516 Acc 75.15% Top5 91.72%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 141/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=51, loss=1.79]  \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 141/200 | Train Loss 1.7903 Acc 50.97% | Val Loss 1.7690 Acc 75.44% Top5 91.12%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 142/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.06it/s, acc=47.5, loss=1.51]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 142/200 | Train Loss 1.5136 Acc 47.50% | Val Loss 1.7805 Acc 73.37% Top5 91.42%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 143/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.99it/s, acc=58.9, loss=1.55]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 143/200 | Train Loss 1.5484 Acc 58.95% | Val Loss 1.7591 Acc 74.85% Top5 90.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 144/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.06it/s, acc=55.2, loss=1.54]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 144/200 | Train Loss 1.5389 Acc 55.20% | Val Loss 1.7528 Acc 75.15% Top5 90.24%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 145/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  2.00it/s, acc=57.3, loss=1.71]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 145/200 | Train Loss 1.7079 Acc 57.28% | Val Loss 1.7598 Acc 74.85% Top5 91.12%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 146/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=50.7, loss=1.43]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 146/200 | Train Loss 1.4346 Acc 50.69% | Val Loss 1.7574 Acc 75.44% Top5 90.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 147/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.96it/s, acc=48.3, loss=1.59]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 147/200 | Train Loss 1.5873 Acc 48.34% | Val Loss 1.7768 Acc 75.74% Top5 91.12%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 148/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=57.1, loss=1.66]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 148/200 | Train Loss 1.6554 Acc 57.07% | Val Loss 1.7888 Acc 74.85% Top5 90.24%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 149/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.05it/s, acc=57.3, loss=1.55]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 149/200 | Train Loss 1.5455 Acc 57.28% | Val Loss 1.7681 Acc 76.33% Top5 90.53%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 150/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.94it/s, acc=59.6, loss=1.65]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:05<00:00,  2.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 150/200 | Train Loss 1.6483 Acc 59.64% | Val Loss 1.7842 Acc 75.74% Top5 90.24%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 151/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  2.00it/s, acc=41.6, loss=1.66]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 151/200 | Train Loss 1.6622 Acc 41.61% | Val Loss 1.7634 Acc 75.15% Top5 90.53%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 152/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.04it/s, acc=42.4, loss=1.69]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 152/200 | Train Loss 1.6925 Acc 42.37% | Val Loss 1.7943 Acc 74.26% Top5 90.24%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 153/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.99it/s, acc=44.1, loss=1.61]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 153/200 | Train Loss 1.6129 Acc 44.11% | Val Loss 1.8071 Acc 73.37% Top5 89.94%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 154/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.00it/s, acc=43.3, loss=1.67]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 154/200 | Train Loss 1.6671 Acc 43.27% | Val Loss 1.7851 Acc 73.96% Top5 89.94%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 155/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.02it/s, acc=61.9, loss=1.75]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 155/200 | Train Loss 1.7514 Acc 61.86% | Val Loss 1.8139 Acc 71.89% Top5 89.35%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 156/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.02it/s, acc=53.1, loss=1.69]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 156/200 | Train Loss 1.6919 Acc 53.12% | Val Loss 1.7657 Acc 75.15% Top5 90.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 157/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.05it/s, acc=53.1, loss=1.73]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 157/200 | Train Loss 1.7349 Acc 53.05% | Val Loss 1.7682 Acc 75.15% Top5 89.94%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 158/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.06it/s, acc=59.6, loss=1.56]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 158/200 | Train Loss 1.5602 Acc 59.64% | Val Loss 1.7694 Acc 76.33% Top5 91.42%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 159/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=48.7, loss=1.66]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 159/200 | Train Loss 1.6600 Acc 48.68% | Val Loss 1.7546 Acc 75.15% Top5 91.12%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 160/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.98it/s, acc=39.9, loss=1.86]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 160/200 | Train Loss 1.8645 Acc 39.88% | Val Loss 1.7687 Acc 76.04% Top5 90.24%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 161/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=46.9, loss=1.65]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 161/200 | Train Loss 1.6495 Acc 46.88% | Val Loss 1.7599 Acc 75.44% Top5 91.12%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 162/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.02it/s, acc=50.1, loss=1.6] \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 162/200 | Train Loss 1.5958 Acc 50.14% | Val Loss 1.7638 Acc 76.63% Top5 90.24%\nâœ… New best model: 76.63%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 163/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.02it/s, acc=49.4, loss=1.76]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 163/200 | Train Loss 1.7559 Acc 49.38% | Val Loss 1.7545 Acc 75.44% Top5 90.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 164/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.04it/s, acc=50.3, loss=1.66]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 164/200 | Train Loss 1.6569 Acc 50.28% | Val Loss 1.7642 Acc 74.26% Top5 90.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 165/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.98it/s, acc=46.3, loss=1.65]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:05<00:00,  2.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 165/200 | Train Loss 1.6515 Acc 46.26% | Val Loss 1.7462 Acc 75.15% Top5 90.53%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 166/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.95it/s, acc=44.7, loss=1.71]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:05<00:00,  2.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 166/200 | Train Loss 1.7058 Acc 44.73% | Val Loss 1.7573 Acc 75.15% Top5 90.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 167/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.97it/s, acc=46.9, loss=1.53]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 167/200 | Train Loss 1.5286 Acc 46.95% | Val Loss 1.7617 Acc 74.56% Top5 90.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 168/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.97it/s, acc=49, loss=1.65]  \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 168/200 | Train Loss 1.6454 Acc 48.96% | Val Loss 1.7547 Acc 75.74% Top5 90.53%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 169/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.04it/s, acc=43.4, loss=1.57]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 169/200 | Train Loss 1.5722 Acc 43.41% | Val Loss 1.7565 Acc 75.15% Top5 90.53%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 170/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.98it/s, acc=46, loss=1.57]  \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:05<00:00,  2.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 170/200 | Train Loss 1.5743 Acc 46.05% | Val Loss 1.7735 Acc 73.08% Top5 91.12%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 171/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.97it/s, acc=52.8, loss=1.7] \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:05<00:00,  2.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 171/200 | Train Loss 1.6984 Acc 52.84% | Val Loss 1.7685 Acc 74.26% Top5 90.53%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 172/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=42, loss=1.71]  \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:05<00:00,  2.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 172/200 | Train Loss 1.7070 Acc 42.02% | Val Loss 1.7761 Acc 74.26% Top5 91.12%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 173/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.98it/s, acc=39.2, loss=1.61]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 173/200 | Train Loss 1.6078 Acc 39.18% | Val Loss 1.7704 Acc 75.44% Top5 90.24%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 174/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.97it/s, acc=42.6, loss=1.53]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 174/200 | Train Loss 1.5254 Acc 42.65% | Val Loss 1.7676 Acc 74.85% Top5 90.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 175/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.98it/s, acc=44.4, loss=1.55]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 175/200 | Train Loss 1.5499 Acc 44.38% | Val Loss 1.7821 Acc 74.26% Top5 91.12%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 176/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.99it/s, acc=52.5, loss=1.56]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 176/200 | Train Loss 1.5558 Acc 52.50% | Val Loss 1.7892 Acc 74.26% Top5 90.53%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 177/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.98it/s, acc=51.5, loss=1.63]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 177/200 | Train Loss 1.6263 Acc 51.46% | Val Loss 1.7484 Acc 76.63% Top5 89.94%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 178/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=51.9, loss=1.63]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 178/200 | Train Loss 1.6297 Acc 51.94% | Val Loss 1.7856 Acc 75.44% Top5 90.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 179/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.02it/s, acc=46.6, loss=1.69]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 179/200 | Train Loss 1.6899 Acc 46.60% | Val Loss 1.7794 Acc 75.44% Top5 90.24%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 180/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.04it/s, acc=63.2, loss=1.64]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 180/200 | Train Loss 1.6448 Acc 63.18% | Val Loss 1.7599 Acc 76.04% Top5 90.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 181/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.01it/s, acc=54.2, loss=1.68]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 181/200 | Train Loss 1.6829 Acc 54.16% | Val Loss 1.7664 Acc 76.04% Top5 91.42%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 182/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=56, loss=1.69]  \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 182/200 | Train Loss 1.6937 Acc 56.03% | Val Loss 1.7555 Acc 76.33% Top5 91.72%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 183/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.01it/s, acc=56, loss=1.61]  \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 183/200 | Train Loss 1.6073 Acc 56.03% | Val Loss 1.7617 Acc 76.04% Top5 91.12%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 184/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.00it/s, acc=43.3, loss=1.53]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 184/200 | Train Loss 1.5325 Acc 43.27% | Val Loss 1.7512 Acc 75.74% Top5 91.12%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 185/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  2.00it/s, acc=60.1, loss=1.62]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 185/200 | Train Loss 1.6180 Acc 60.06% | Val Loss 1.7530 Acc 76.33% Top5 90.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 186/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=57.3, loss=1.69]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 186/200 | Train Loss 1.6943 Acc 57.28% | Val Loss 1.7635 Acc 76.33% Top5 91.42%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 187/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.97it/s, acc=62.6, loss=1.58]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:05<00:00,  2.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 187/200 | Train Loss 1.5773 Acc 62.55% | Val Loss 1.7729 Acc 74.26% Top5 91.72%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 188/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.97it/s, acc=53.3, loss=1.59]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 188/200 | Train Loss 1.5889 Acc 53.33% | Val Loss 1.7714 Acc 75.15% Top5 90.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 189/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.99it/s, acc=45.4, loss=1.48]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 189/200 | Train Loss 1.4792 Acc 45.35% | Val Loss 1.7596 Acc 76.04% Top5 91.72%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 190/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.02it/s, acc=44.5, loss=1.8] \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 190/200 | Train Loss 1.8015 Acc 44.52% | Val Loss 1.7642 Acc 76.04% Top5 91.12%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 191/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.02it/s, acc=61.7, loss=1.7] \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 191/200 | Train Loss 1.7008 Acc 61.72% | Val Loss 1.7729 Acc 74.85% Top5 90.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 192/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.05it/s, acc=56.4, loss=1.62]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 192/200 | Train Loss 1.6206 Acc 56.38% | Val Loss 1.7741 Acc 75.15% Top5 90.24%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 193/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.02it/s, acc=61.4, loss=1.67]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 193/200 | Train Loss 1.6687 Acc 61.37% | Val Loss 1.7896 Acc 75.15% Top5 90.53%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 194/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.02it/s, acc=55, loss=1.6]   \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 194/200 | Train Loss 1.6034 Acc 54.99% | Val Loss 1.7766 Acc 74.56% Top5 91.12%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 195/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.03it/s, acc=48.1, loss=1.56]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 195/200 | Train Loss 1.5584 Acc 48.13% | Val Loss 1.7633 Acc 76.33% Top5 90.53%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 196/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.02it/s, acc=51.5, loss=1.6] \nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 196/200 | Train Loss 1.5957 Acc 51.46% | Val Loss 1.7885 Acc 74.56% Top5 90.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 197/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.04it/s, acc=50.1, loss=1.61]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 197/200 | Train Loss 1.6099 Acc 50.14% | Val Loss 1.7698 Acc 75.15% Top5 90.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 198/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:22<00:00,  2.07it/s, acc=58.7, loss=1.63]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 198/200 | Train Loss 1.6280 Acc 58.67% | Val Loss 1.7803 Acc 74.56% Top5 90.53%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 199/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.99it/s, acc=60.8, loss=1.41]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 199/200 | Train Loss 1.4118 Acc 60.82% | Val Loss 1.7626 Acc 74.26% Top5 91.72%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 200/200 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:23<00:00,  1.99it/s, acc=41.2, loss=1.46]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 200/200 | Train Loss 1.4591 Acc 41.19% | Val Loss 1.7536 Acc 75.44% Top5 91.42%\nTraining complete. Best val: 76.62721866404517\nTraining log saved to training_log.csv\nTraining curves saved to training_curves.png\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:03<00:00,  2.40it/s]","output_type":"stream"},{"name":"stdout","text":"Final Test Accuracy: 72.09% Top5: 88.76%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}