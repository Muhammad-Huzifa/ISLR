{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ff041c9",
   "metadata": {},
   "source": [
    "### 1. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad908521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1c28d3",
   "metadata": {},
   "source": [
    "### 2. Configuration (Centralized Settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54359bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Path to the folder containing the WLASL video files\n",
    "    \"VIDEO_SOURCE_DIR\": r\"../data/wlasl-complete/videos\",\n",
    "\n",
    "    # Path to the JSON file that defines the dataset splits\n",
    "    \"SPLIT_FILE_PATH\": r\"../data/nslt_300.json\",\n",
    "\n",
    "    # Path where the final landmark .npz file will be saved\n",
    "    \"OUTPUT_NPZ_PATH\": r\"../data/Landmarks_GCN_augmented.npz\",\n",
    "\n",
    "    # How often to save a backup (every 250 videos)\n",
    "    \"SAVE_CHECKPOINT_EVERY_N_VIDEOS\": 250,\n",
    "\n",
    "    # --- MediaPipe Node Selection ---\n",
    "    # We select specific pose landmarks to reduce data size and focus on\n",
    "    # key points for sign language (arms, shoulders, etc.)\n",
    "    \"POSE_NODES\": [\n",
    "        mp.solutions.holistic.PoseLandmark.NOSE,\n",
    "        mp.solutions.holistic.PoseLandmark.LEFT_SHOULDER,\n",
    "        mp.solutions.holistic.PoseLandmark.RIGHT_SHOULDER,\n",
    "        mp.solutions.holistic.PoseLandmark.LEFT_ELBOW,\n",
    "        mp.solutions.holistic.PoseLandmark.RIGHT_ELBOW,\n",
    "        mp.solutions.holistic.PoseLandmark.LEFT_WRIST,\n",
    "        mp.solutions.holistic.PoseLandmark.RIGHT_WRIST,\n",
    "        mp.solutions.holistic.PoseLandmark.LEFT_HIP,\n",
    "        mp.solutions.holistic.PoseLandmark.RIGHT_HIP,\n",
    "        mp.solutions.holistic.PoseLandmark.LEFT_KNEE,\n",
    "        mp.solutions.holistic.PoseLandmark.RIGHT_KNEE,\n",
    "        mp.solutions.holistic.PoseLandmark.LEFT_ANKLE,\n",
    "        mp.solutions.holistic.PoseLandmark.RIGHT_ANKLE,\n",
    "        mp.solutions.holistic.PoseLandmark.LEFT_INDEX,\n",
    "        mp.solutions.holistic.PoseLandmark.RIGHT_INDEX\n",
    "    ],\n",
    "\n",
    "    # We select groups of face landmarks (lips, eyes)\n",
    "    \"FACE_GROUPS\": [\n",
    "        mp.solutions.face_mesh.FACEMESH_LIPS,\n",
    "        mp.solutions.face_mesh.FACEMESH_LEFT_EYE,\n",
    "        mp.solutions.face_mesh.FACEMESH_RIGHT_EYE,\n",
    "        mp.solutions.face_mesh.FACEMESH_LEFT_IRIS if hasattr(mp.solutions.face_mesh, \"FACEMESH_LEFT_IRIS\") else (),\n",
    "        mp.solutions.face_mesh.FACEMESH_RIGHT_IRIS if hasattr(mp.solutions.face_mesh, \"FACEMESH_RIGHT_IRIS\") else (),\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b59f7e",
   "metadata": {},
   "source": [
    "### 3. Initialize MediaPipe Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0be1be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the main tools we'll use from MediaPipe\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82183d24",
   "metadata": {},
   "source": [
    "### --- 4. Helper Functions ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de61d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_to_xyzc(lm, default_conf=1.0):\n",
    "    \"\"\"\n",
    "    Converts a MediaPipe landmark object into a standard [x, y, z, confidence] list.\n",
    "\n",
    "    Args:\n",
    "        lm (mediapipe.framework.formats.landmark_pb2.Landmark): The landmark object.\n",
    "        default_conf (float): The confidence value to use if 'visibility' is not available.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing [x, y, z, confidence].\n",
    "    \"\"\"\n",
    "    x = lm.x if hasattr(lm, 'x') else 0.0\n",
    "    y = lm.y if hasattr(lm, 'y') else 0.0\n",
    "    z = lm.z if hasattr(lm, 'z') else 0.0\n",
    "    \n",
    "    # Use 'visibility' as the confidence score, if it exists\n",
    "    conf = getattr(lm, 'visibility', None)\n",
    "    if conf is None:\n",
    "        conf = default_conf\n",
    "        \n",
    "    return [x, y, z, conf]\n",
    "\n",
    "\n",
    "def sample_face_indices():\n",
    "    \"\"\"\n",
    "    Selects a small, representative sample of 8 face landmarks.\n",
    "    \n",
    "    The full face mesh has 468 points, which is too much data. We sample\n",
    "    from the groups defined in our config (lips, eyes) to get a small\n",
    "    but useful subset.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of 8 landmark indices to keep.\n",
    "    \"\"\"\n",
    "    uniq = []\n",
    "    for group in config[\"FACE_GROUPS\"]:\n",
    "        if not group:\n",
    "            continue\n",
    "        for pair in group:\n",
    "            if isinstance(pair, (tuple, list)) and len(pair) >= 2:\n",
    "                for idx in pair[:2]:\n",
    "                    if idx not in uniq:\n",
    "                        uniq.append(idx)\n",
    "                        \n",
    "    # If not enough points, just take the first 8\n",
    "    if len(uniq) < 8:\n",
    "        uniq = list(range(8))\n",
    "        \n",
    "    # Sample 8 points evenly from the unique list\n",
    "    step = max(1, len(uniq) // 8)\n",
    "    chosen = uniq[0:8*step:step][:8]\n",
    "    \n",
    "    return chosen\n",
    "\n",
    "# Pre-calculate the face indices we will keep. This runs only once.\n",
    "FACE_INDICES_TO_KEEP = sample_face_indices()\n",
    "\n",
    "\n",
    "def build_adjacency_map(node_labels):\n",
    "    \"\"\"\n",
    "    Builds an (N, N) adjacency matrix for the graph nodes.\n",
    "    \n",
    "    This matrix defines which nodes (landmarks) are connected to each other,\n",
    "    forming the \"skeleton\" of the sign language performer.\n",
    "\n",
    "    Args:\n",
    "        node_labels (list): A list of tuples, e.g., [('pose','LEFT_WRIST'), ...].\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An (N, N) binary adjacency matrix.\n",
    "    \"\"\"\n",
    "    N = len(node_labels)\n",
    "    adj = np.zeros((N, N), dtype=np.uint8)\n",
    "    # Create a quick lookup map: {'pose_LEFT_WRIST': 5, ...}\n",
    "    label_to_idx = {lbl: i for i, lbl in enumerate(node_labels)}\n",
    "\n",
    "    # --- 1. Connect standard pose landmarks (e.g., shoulder-to-elbow) ---\n",
    "    if hasattr(mp.solutions.pose, \"POSE_CONNECTIONS\"):\n",
    "        pose_conn = mp.solutions.pose.POSE_CONNECTIONS\n",
    "    else:\n",
    "        pose_conn = ()\n",
    "\n",
    "    for (a, b) in pose_conn:\n",
    "        a_name = mp_holistic.PoseLandmark(a).name if isinstance(a, int) else None\n",
    "        b_name = mp_holistic.PoseLandmark(b).name if isinstance(b, int) else None\n",
    "        key_a = ('pose', a_name)\n",
    "        key_b = ('pose', b_name)\n",
    "        if key_a in label_to_idx and key_b in label_to_idx:\n",
    "            ia, ib = label_to_idx[key_a], label_to_idx[key_b]\n",
    "            adj[ia, ib] = 1\n",
    "            adj[ib, ia] = 1\n",
    "\n",
    "    # --- 2. Connect our specific list of POSE_NODES sequentially ---\n",
    "    pose_node_indices = [label_to_idx.get(('pose', p.name)) for p in config[\"POSE_NODES\"]]\n",
    "    pose_node_indices = [i for i in pose_node_indices if i is not None]\n",
    "    for i in range(len(pose_node_indices)-1):\n",
    "        a, b = pose_node_indices[i], pose_node_indices[i+1]\n",
    "        adj[a, b] = 1\n",
    "        adj[b, a] = 1\n",
    "\n",
    "    # --- 3. Connect midpoints (shoulder/hip) to their endpoints ---\n",
    "    for mid_name, endpoints in [('mid_shoulder', ('LEFT_SHOULDER','RIGHT_SHOULDER')),\n",
    "                                 ('mid_hip', ('LEFT_HIP','RIGHT_HIP'))]:\n",
    "        mid_key = ('pose_mid', mid_name)\n",
    "        if mid_key in label_to_idx:\n",
    "            mid_i = label_to_idx[mid_key]\n",
    "            for ep in endpoints:\n",
    "                ep_key = ('pose', ep)\n",
    "                if ep_key in label_to_idx:\n",
    "                    ei = label_to_idx[ep_key]\n",
    "                    adj[mid_i, ei] = 1\n",
    "                    adj[ei, mid_i] = 1\n",
    "\n",
    "    # --- 4. Connect hand landmarks (e.g., finger bones) ---\n",
    "    if hasattr(mp_hands, \"HAND_CONNECTIONS\"):\n",
    "        for hand_side in ('L','R'):\n",
    "            for (a, b) in mp_hands.HAND_CONNECTIONS:\n",
    "                node_a = ('hand', f\"{hand_side}_{a}\")\n",
    "                node_b = ('hand', f\"{hand_side}_{b}\")\n",
    "                if node_a in label_to_idx and node_b in label_to_idx:\n",
    "                    ia, ib = label_to_idx[node_a], label_to_idx[node_b]\n",
    "                    adj[ia, ib] = 1\n",
    "                    adj[ib, ia] = 1\n",
    "\n",
    "    # --- 5. Connect hands to wrists ---\n",
    "    for side, pose_label in [('L','LEFT_WRIST'), ('R','RIGHT_WRIST')]:\n",
    "        pose_key = ('pose', pose_label)\n",
    "        hand_key = ('hand', f\"{side}_0\") # _0 is the hand's wrist landmark\n",
    "        if pose_key in label_to_idx and hand_key in label_to_idx:\n",
    "            pi = label_to_idx[pose_key]\n",
    "            hi = label_to_idx[hand_key]\n",
    "            adj[pi, hi] = 1\n",
    "            adj[hi, pi] = 1\n",
    "\n",
    "    # --- 6. Connect face points to the nose (as a central point) ---\n",
    "    nose_key = ('pose', 'NOSE')\n",
    "    if nose_key in label_to_idx:\n",
    "        ni = label_to_idx[nose_key]\n",
    "        for fi, lbl in enumerate(node_labels):\n",
    "            if lbl[0] == 'face': # if it's a face landmark\n",
    "                fi_idx = label_to_idx[lbl]\n",
    "                adj[ni, fi_idx] = 1\n",
    "                adj[fi_idx, ni] = 1\n",
    "\n",
    "    return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f5d864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_compact_landmarks(video_path, holistic_model):\n",
    "    \"\"\"\n",
    "    Processes a single video file and extracts all landmark data.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): The file path to the video.\n",
    "        holistic_model (mediapipe.solutions.holistic.Holistic): The initialized MediaPipe model.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (landmark_array, torso_array, bone_array)\n",
    "            - landmark_array (T, N, 4): (x,y,z,conf) for T frames, N nodes\n",
    "            - torso_array (T, 1): Torso length for each frame\n",
    "            - bone_array (T, M*3): Bone vectors for each frame\n",
    "            Returns (None, None, None) on failure.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Could not open video file: {video_path}\")\n",
    "        return None, None, None\n",
    "\n",
    "    frames = []\n",
    "    torso_lengths = []\n",
    "    bone_vectors_per_frame = []\n",
    "\n",
    "    try:\n",
    "        while cap.isOpened():\n",
    "            # Read a frame from the video\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break # End of video\n",
    "                \n",
    "            # --- MediaPipe Processing ---\n",
    "            # Convert BGR (OpenCV default) to RGB (MediaPipe default)\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False # Make read-only for performance\n",
    "            # Run landmark detection\n",
    "            results = holistic_model.process(image)\n",
    "            image.flags.writeable = True # Make writable again\n",
    "\n",
    "            node_features = []\n",
    "            frame_torso_length = 0.0\n",
    "            frame_bone_vectors = []\n",
    "\n",
    "            # --- 1. Extract Pose Landmarks (and midpoints) ---\n",
    "            if results.pose_landmarks:\n",
    "                # Add the specific nodes from our config\n",
    "                for p in config[\"POSE_NODES\"]:\n",
    "                    lm = results.pose_landmarks.landmark[p]\n",
    "                    node_features.append(lm_to_xyzc(lm, default_conf=getattr(lm, 'visibility', 1.0)))\n",
    "                \n",
    "                # --- Calculate and add midpoints ---\n",
    "                left_sh = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_SHOULDER]\n",
    "                right_sh = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_SHOULDER]\n",
    "                mid_sh = [(left_sh.x + right_sh.x)/2, (left_sh.y + right_sh.y)/2, (left_sh.z + right_sh.z)/2, 1.0]\n",
    "                node_features.append(mid_sh)\n",
    "\n",
    "                left_hip = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_HIP]\n",
    "                right_hip = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_HIP]\n",
    "                mid_hip = [(left_hip.x + right_hip.x)/2, (left_hip.y + right_hip.y)/2, (left_hip.z + right_hip.z)/2, 1.0]\n",
    "                node_features.append(mid_hip)\n",
    "                \n",
    "                # --- 2. Calculate Torso Length (for normalization) ---\n",
    "                # We use the distance between mid-shoulder and mid-hip.\n",
    "                # This is a stable way to measure torso size.\n",
    "                dx = mid_sh[0] - mid_hip[0]\n",
    "                dy = mid_sh[1] - mid_hip[1]\n",
    "                dz = mid_sh[2] - mid_hip[2]\n",
    "                frame_torso_length = sqrt(dx**2 + dy**2 + dz**2)\n",
    "            else:\n",
    "                # If no pose is detected, pad with zeros\n",
    "                num_pose_nodes = len(config[\"POSE_NODES\"]) + 2 # +2 for midpoints\n",
    "                for _ in range(num_pose_nodes):\n",
    "                    node_features.append([0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "            # --- 3. Extract Left Hand Landmarks (21 nodes) ---\n",
    "            if results.left_hand_landmarks:\n",
    "                for lm in results.left_hand_landmarks.landmark:\n",
    "                    node_features.append(lm_to_xyzc(lm, default_conf=1.0))\n",
    "            else:\n",
    "                # Pad with zeros if no hand detected\n",
    "                for _ in range(21):\n",
    "                    node_features.append([0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "            # --- 4. Extract Right Hand Landmarks (21 nodes) ---\n",
    "            if results.right_hand_landmarks:\n",
    "                for lm in results.right_hand_landmarks.landmark:\n",
    "                    node_features.append(lm_to_xyzc(lm, default_conf=1.0))\n",
    "            else:\n",
    "                # Pad with zeros if no hand detected\n",
    "                for _ in range(21):\n",
    "                    node_features.append([0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "            # --- 5. Extract Face Landmarks (8 sampled nodes) ---\n",
    "            if results.face_landmarks:\n",
    "                for idx in FACE_INDICES_TO_KEEP:\n",
    "                    if idx < len(results.face_landmarks.landmark):\n",
    "                        lm = results.face_landmarks.landmark[idx]\n",
    "                        node_features.append(lm_to_xyzc(lm, default_conf=1.0))\n",
    "                    else:\n",
    "                        node_features.append([0.0, 0.0, 0.0, 0.0])\n",
    "            else:\n",
    "                # Pad with zeros if no face detected\n",
    "                for _ in range(len(FACE_INDICES_TO_KEEP)):\n",
    "                    node_features.append([0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "            # --- 6. Calculate Bone Vectors ---\n",
    "            # \"Bones\" are vectors (B - A) between key joints.\n",
    "            # This captures motion *direction* in addition to joint *position*.\n",
    "            if results.pose_landmarks:\n",
    "                lm = results.pose_landmarks.landmark\n",
    "                # Right Arm (shoulder->elbow, elbow->wrist)\n",
    "                for start, end in [(mp_holistic.PoseLandmark.RIGHT_SHOULDER, mp_holistic.PoseLandmark.RIGHT_ELBOW),\n",
    "                                   (mp_holistic.PoseLandmark.RIGHT_ELBOW, mp_holistic.PoseLandmark.RIGHT_WRIST)]:\n",
    "                    vec = [lm[end].x - lm[start].x, lm[end].y - lm[start].y, lm[end].z - lm[start].z]\n",
    "                    frame_bone_vectors.extend(vec)\n",
    "                # Left Arm (shoulder->elbow, elbow->wrist)\n",
    "                for start, end in [(mp_holistic.PoseLandmark.LEFT_SHOULDER, mp_holistic.PoseLandmark.LEFT_ELBOW),\n",
    "                                   (mp_holistic.PoseLandmark.LEFT_ELBOW, mp_holistic.PoseLandmark.LEFT_WRIST)]:\n",
    "                    vec = [lm[end].x - lm[start].x, lm[end].y - lm[start].y, lm[end].z - lm[start].z]\n",
    "                    frame_bone_vectors.extend(vec)\n",
    "            else:\n",
    "                # Pad with zeros if no pose\n",
    "                frame_bone_vectors = [0.0] * 12 # 4 bones * 3 axes (x,y,z)\n",
    "            \n",
    "            # Add all data for this frame to our lists\n",
    "            frames.append(np.array(node_features, dtype=np.float32))\n",
    "            torso_lengths.append(frame_torso_length)\n",
    "            bone_vectors_per_frame.append(frame_bone_vectors)\n",
    "\n",
    "    finally:\n",
    "        # Always release the video capture\n",
    "        cap.release()\n",
    "\n",
    "    if not frames:\n",
    "        return None, None, None\n",
    "    \n",
    "    # --- Final Step: Stack all frame data into large NumPy arrays ---\n",
    "    landmark_seq = np.stack(frames, axis=0)\n",
    "    torso_seq = np.array(torso_lengths, dtype=np.float32)\n",
    "    bone_seq = np.array(bone_vectors_per_frame, dtype=np.float32)\n",
    "    \n",
    "    return landmark_seq, torso_seq, bone_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f12312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the landmark extraction pipeline.\n",
    "    \n",
    "    1. Loads the video list from the JSON file.\n",
    "    2. Defines the complete list of node labels.\n",
    "    3. Builds the adjacency matrix.\n",
    "    4. Initializes the MediaPipe Holistic model.\n",
    "    5. Loops through all videos, extracts landmarks, and saves them.\n",
    "    6. Saves checkpoints and a final .npz file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Load video list ---\n",
    "    print(f\"Loading video list from: {config['SPLIT_FILE_PATH']}\")\n",
    "    with open(config[\"SPLIT_FILE_PATH\"], 'r') as f:\n",
    "        data = json.load(f)\n",
    "    video_ids = list(data.keys())\n",
    "    # Create a list of (video_id, full_video_path)\n",
    "    video_paths = [(vid, os.path.join(config[\"VIDEO_SOURCE_DIR\"], f\"{vid}.mp4\")) for vid in video_ids]\n",
    "    print(f\"Found {len(video_paths)} videos to process.\")\n",
    "\n",
    "    # --- 2. Define node labels ---\n",
    "    # This list MUST match the order of extraction in extract_compact_landmarks\n",
    "    node_labels = []\n",
    "    # Pose nodes (from config)\n",
    "    for p in config[\"POSE_NODES\"]:\n",
    "        node_labels.append(('pose', p.name))\n",
    "    # Pose midpoints\n",
    "    node_labels.append(('pose_mid', 'mid_shoulder'))\n",
    "    node_labels.append(('pose_mid', 'mid_hip'))\n",
    "    # Left hand\n",
    "    for i in range(21):\n",
    "        node_labels.append(('hand', f\"L_{i}\"))\n",
    "    # Right hand\n",
    "    for i in range(21):\n",
    "        node_labels.append(('hand', f\"R_{i}\"))\n",
    "    # Face nodes\n",
    "    for idx in FACE_INDICES_TO_KEEP:\n",
    "        node_labels.append(('face', f\"F_{idx}\"))\n",
    "    \n",
    "    print(f\"Total nodes defined: {len(node_labels)}\")\n",
    "\n",
    "    # --- 3. Build the adjacency map ---\n",
    "    print(\"Building adjacency map...\")\n",
    "    adjacency = build_adjacency_map(node_labels)\n",
    "\n",
    "    # --- 4. Initialize MediaPipe Holistic ---\n",
    "    holistic = mp_holistic.Holistic(static_image_mode=False,\n",
    "                                    model_complexity=1,\n",
    "                                    min_detection_confidence=0.5,\n",
    "                                    min_tracking_confidence=0.5)\n",
    "\n",
    "    # --- 5. Run processing loop ---\n",
    "    # These dictionaries will store the data: {'video_id': np_array, ...}\n",
    "    extracted_landmarks = {}\n",
    "    extracted_torsos = {}\n",
    "    extracted_bones = {}\n",
    "\n",
    "    print(\"Starting landmark extraction...\")\n",
    "    try:\n",
    "        for i, (vid, path) in enumerate(tqdm(video_paths, desc=\"Processing Videos\")):\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"Warning: Video file not found, skipping: {path}\")\n",
    "                continue\n",
    "                \n",
    "            # This is where the core extraction happens\n",
    "            seq, torso_len, bone_vec = extract_compact_landmarks(path, holistic)\n",
    "            \n",
    "            if seq is not None:\n",
    "                extracted_landmarks[vid] = seq\n",
    "                extracted_torsos[vid] = torso_len\n",
    "                extracted_bones[vid] = bone_vec\n",
    "                \n",
    "            # --- 6. Save checkpoint ---\n",
    "            if (i > 0) and (i+1) % config[\"SAVE_CHECKPOINT_EVERY_N_VIDEOS\"] == 0:\n",
    "                print(f\"\\n--- Saving checkpoint with {len(extracted_landmarks)} videos ---\")\n",
    "                np.savez_compressed(config[\"OUTPUT_NPZ_PATH\"],\n",
    "                                     _node_labels=np.array(node_labels, dtype=object),\n",
    "                                     _adjacency=adjacency,\n",
    "                                     _torso_lengths=extracted_torsos,\n",
    "                                     _bone_vectors=extracted_bones,\n",
    "                                     **extracted_landmarks)\n",
    "    finally:\n",
    "        # Always close the MediaPipe model\n",
    "        holistic.close()\n",
    "\n",
    "    # --- 7. Save final file ---\n",
    "    print(f\"\\nExtraction complete. Saving final file to: {config['OUTPUT_NPZ_PATH']}\")\n",
    "    np.savez_compressed(config[\"OUTPUT_NPZ_PATH\"],\n",
    "                        _node_labels=np.array(node_labels, dtype=object),\n",
    "                        _adjacency=adjacency,\n",
    "                        _torso_lengths=extracted_torsos,\n",
    "                        _bone_vectors=extracted_bones,\n",
    "                        **extracted_landmarks)\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465680b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:   5%|▍         | 249/5118 [18:26<5:16:31,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 250 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  10%|▉         | 499/5118 [1:17:22<2:45:26,  2.15s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 500 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  15%|█▍        | 749/5118 [1:29:24<4:05:33,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 750 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  20%|█▉        | 999/5118 [1:48:53<7:36:00,  6.64s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 1000 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  24%|██▍       | 1249/5118 [2:17:13<7:55:21,  7.37s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 1250 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  29%|██▉       | 1499/5118 [2:46:21<4:27:23,  4.43s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 1500 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  34%|███▍      | 1749/5118 [3:03:33<3:43:09,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 1750 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  39%|███▉      | 1999/5118 [3:20:25<4:53:33,  5.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 2000 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  44%|████▍     | 2249/5118 [3:34:27<1:59:30,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 2250 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  49%|████▉     | 2499/5118 [3:48:55<2:24:02,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 2500 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  54%|█████▎    | 2749/5118 [4:04:45<2:15:23,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 2750 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  59%|█████▊    | 2999/5118 [4:20:05<2:03:45,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 3000 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  63%|██████▎   | 3249/5118 [4:35:23<2:10:17,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 3250 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  68%|██████▊   | 3499/5118 [4:51:51<1:54:02,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 3500 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  73%|███████▎  | 3749/5118 [5:07:52<1:07:52,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 3750 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  78%|███████▊  | 3999/5118 [5:23:31<1:01:22,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 4000 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  83%|████████▎ | 4249/5118 [5:39:31<52:15,  3.61s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 4250 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  88%|████████▊ | 4499/5118 [5:53:43<39:25,  3.82s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 4500 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  93%|█████████▎| 4749/5118 [6:10:34<20:44,  3.37s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 4750 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  98%|█████████▊| 4999/5118 [6:23:52<07:09,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 5000 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos: 100%|██████████| 5118/5118 [6:30:59<00:00,  4.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Landmarks\\Bones+Joints\\Landmarks_GCN_augmented.npz\n"
     ]
    }
   ],
   "source": [
    "# This guard ensures that main() only runs when you execute the script directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0b1be1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
