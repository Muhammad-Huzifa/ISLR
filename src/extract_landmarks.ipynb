{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "465680b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:   5%|▍         | 249/5118 [18:26<5:16:31,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 250 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  10%|▉         | 499/5118 [1:17:22<2:45:26,  2.15s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 500 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  15%|█▍        | 749/5118 [1:29:24<4:05:33,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 750 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  20%|█▉        | 999/5118 [1:48:53<7:36:00,  6.64s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 1000 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  24%|██▍       | 1249/5118 [2:17:13<7:55:21,  7.37s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 1250 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  29%|██▉       | 1499/5118 [2:46:21<4:27:23,  4.43s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 1500 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  34%|███▍      | 1749/5118 [3:03:33<3:43:09,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 1750 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  39%|███▉      | 1999/5118 [3:20:25<4:53:33,  5.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 2000 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  44%|████▍     | 2249/5118 [3:34:27<1:59:30,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 2250 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  49%|████▉     | 2499/5118 [3:48:55<2:24:02,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 2500 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  54%|█████▎    | 2749/5118 [4:04:45<2:15:23,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 2750 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  59%|█████▊    | 2999/5118 [4:20:05<2:03:45,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 3000 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  63%|██████▎   | 3249/5118 [4:35:23<2:10:17,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 3250 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  68%|██████▊   | 3499/5118 [4:51:51<1:54:02,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 3500 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  73%|███████▎  | 3749/5118 [5:07:52<1:07:52,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 3750 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  78%|███████▊  | 3999/5118 [5:23:31<1:01:22,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 4000 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  83%|████████▎ | 4249/5118 [5:39:31<52:15,  3.61s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 4250 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  88%|████████▊ | 4499/5118 [5:53:43<39:25,  3.82s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 4500 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  93%|█████████▎| 4749/5118 [6:10:34<20:44,  3.37s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 4750 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos:  98%|█████████▊| 4999/5118 [6:23:52<07:09,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint with 5000 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos: 100%|██████████| 5118/5118 [6:30:59<00:00,  4.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Landmarks\\Bones+Joints\\Landmarks_GCN_augmented.npz\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from math import sqrt\n",
    "\n",
    "# --- Configuration (Centralized Settings) ---\n",
    "config = {\n",
    "    \"VIDEO_SOURCE_DIR\": r\"D:\\FYP\\Dataset\\wlasl-complete\\videos\",\n",
    "    \"SPLIT_FILE_PATH\": r\"D:\\WLASL DataSet\\WLASL FULL\\nslt_300.json\",\n",
    "    \"OUTPUT_NPZ_PATH\": r\"D:\\Landmarks\\Bones+Joints\\Landmarks_GCN_augmented.npz\",\n",
    "    \"SAVE_CHECKPOINT_EVERY_N_VIDEOS\": 250,\n",
    "    \"POSE_NODES\": [\n",
    "        mp.solutions.holistic.PoseLandmark.NOSE,\n",
    "        mp.solutions.holistic.PoseLandmark.LEFT_SHOULDER,\n",
    "        mp.solutions.holistic.PoseLandmark.RIGHT_SHOULDER,\n",
    "        mp.solutions.holistic.PoseLandmark.LEFT_ELBOW,\n",
    "        mp.solutions.holistic.PoseLandmark.RIGHT_ELBOW,\n",
    "        mp.solutions.holistic.PoseLandmark.LEFT_WRIST,\n",
    "        mp.solutions.holistic.PoseLandmark.RIGHT_WRIST,\n",
    "        mp.solutions.holistic.PoseLandmark.LEFT_HIP,\n",
    "        mp.solutions.holistic.PoseLandmark.RIGHT_HIP,\n",
    "        mp.solutions.holistic.PoseLandmark.LEFT_KNEE,\n",
    "        mp.solutions.holistic.PoseLandmark.RIGHT_KNEE,\n",
    "        mp.solutions.holistic.PoseLandmark.LEFT_ANKLE,\n",
    "        mp.solutions.holistic.PoseLandmark.RIGHT_ANKLE,\n",
    "        mp.solutions.holistic.PoseLandmark.LEFT_INDEX,\n",
    "        mp.solutions.holistic.PoseLandmark.RIGHT_INDEX\n",
    "    ],\n",
    "    \"FACE_GROUPS\": [\n",
    "        mp.solutions.face_mesh.FACEMESH_LIPS,\n",
    "        mp.solutions.face_mesh.FACEMESH_LEFT_EYE,\n",
    "        mp.solutions.face_mesh.FACEMESH_RIGHT_EYE,\n",
    "        mp.solutions.face_mesh.FACEMESH_LEFT_IRIS if hasattr(mp.solutions.face_mesh, \"FACEMESH_LEFT_IRIS\") else (),\n",
    "        mp.solutions.face_mesh.FACEMESH_RIGHT_IRIS if hasattr(mp.solutions.face_mesh, \"FACEMESH_RIGHT_IRIS\") else (),\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- Node selection choices ---\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Utilities\n",
    "def lm_to_xyzc(lm, default_conf=1.0):\n",
    "    \"\"\"Return (x,y,z,confidence) given a mediapipe landmark. Some landmarks have 'visibility' or not.\"\"\"\n",
    "    x = lm.x if hasattr(lm, 'x') else 0.0\n",
    "    y = lm.y if hasattr(lm, 'y') else 0.0\n",
    "    z = lm.z if hasattr(lm, 'z') else 0.0\n",
    "    conf = getattr(lm, 'visibility', None)\n",
    "    if conf is None:\n",
    "        conf = default_conf\n",
    "    return [x, y, z, conf]\n",
    "\n",
    "def sample_face_indices():\n",
    "    \"\"\"Take the union of indices from chosen FACE_GROUPS and pick representative indices (~8).\"\"\"\n",
    "    uniq = []\n",
    "    for group in config[\"FACE_GROUPS\"]:\n",
    "        if not group:\n",
    "            continue\n",
    "        for pair in group:\n",
    "            if isinstance(pair, (tuple, list)) and len(pair) >= 2:\n",
    "                for idx in pair[:2]:\n",
    "                    if idx not in uniq:\n",
    "                        uniq.append(idx)\n",
    "    if len(uniq) < 8:\n",
    "        uniq = list(range(8))\n",
    "    step = max(1, len(uniq) // 8)\n",
    "    chosen = uniq[0:8*step:step][:8]\n",
    "    return chosen\n",
    "\n",
    "# Precompute face indices we will keep\n",
    "FACE_INDICES_TO_KEEP = sample_face_indices()\n",
    "\n",
    "def build_adjacency_map(node_labels):\n",
    "    \"\"\"\n",
    "    node_labels: list of tuples (type, sublabel) to identify each node, e.g.\n",
    "      ('pose','LEFT_WRIST'), ('pose_mid','mid_shoulder'), ('hand','L_0'), etc.\n",
    "    Returns adjacency as an (N,N) binary numpy array.\n",
    "    \"\"\"\n",
    "    N = len(node_labels)\n",
    "    adj = np.zeros((N, N), dtype=np.uint8)\n",
    "    label_to_idx = {lbl: i for i, lbl in enumerate(node_labels)}\n",
    "\n",
    "    if hasattr(mp.solutions.pose, \"POSE_CONNECTIONS\"):\n",
    "        pose_conn = mp.solutions.pose.POSE_CONNECTIONS\n",
    "    else:\n",
    "        pose_conn = ()\n",
    "\n",
    "    for (a, b) in pose_conn:\n",
    "        a_name = mp_holistic.PoseLandmark(a).name if isinstance(a, int) else None\n",
    "        b_name = mp_holistic.PoseLandmark(b).name if isinstance(b, int) else None\n",
    "        key_a = ('pose', a_name)\n",
    "        key_b = ('pose', b_name)\n",
    "        if key_a in label_to_idx and key_b in label_to_idx:\n",
    "            ia, ib = label_to_idx[key_a], label_to_idx[key_b]\n",
    "            adj[ia, ib] = 1\n",
    "            adj[ib, ia] = 1\n",
    "\n",
    "    pose_node_indices = [label_to_idx.get(('pose', p.name)) for p in config[\"POSE_NODES\"]]\n",
    "    pose_node_indices = [i for i in pose_node_indices if i is not None]\n",
    "    for i in range(len(pose_node_indices)-1):\n",
    "        a, b = pose_node_indices[i], pose_node_indices[i+1]\n",
    "        adj[a, b] = 1\n",
    "        adj[b, a] = 1\n",
    "\n",
    "    for mid_name, endpoints in [('mid_shoulder', ('LEFT_SHOULDER','RIGHT_SHOULDER')),\n",
    "                                 ('mid_hip', ('LEFT_HIP','RIGHT_HIP'))]:\n",
    "        mid_key = ('pose_mid', mid_name)\n",
    "        if mid_key in label_to_idx:\n",
    "            mid_i = label_to_idx[mid_key]\n",
    "            for ep in endpoints:\n",
    "                ep_key = ('pose', ep)\n",
    "                if ep_key in label_to_idx:\n",
    "                    ei = label_to_idx[ep_key]\n",
    "                    adj[mid_i, ei] = 1\n",
    "                    adj[ei, mid_i] = 1\n",
    "\n",
    "    if hasattr(mp_hands, \"HAND_CONNECTIONS\"):\n",
    "        for hand_side in ('L','R'):\n",
    "            for (a, b) in mp_hands.HAND_CONNECTIONS:\n",
    "                node_a = ('hand', f\"{hand_side}_{a}\")\n",
    "                node_b = ('hand', f\"{hand_side}_{b}\")\n",
    "                if node_a in label_to_idx and node_b in label_to_idx:\n",
    "                    ia, ib = label_to_idx[node_a], label_to_idx[node_b]\n",
    "                    adj[ia, ib] = 1\n",
    "                    adj[ib, ia] = 1\n",
    "\n",
    "    for side, pose_label in [('L','LEFT_WRIST'), ('R','RIGHT_WRIST')]:\n",
    "        pose_key = ('pose', pose_label)\n",
    "        hand_key = ('hand', f\"{side}_0\")\n",
    "        if pose_key in label_to_idx and hand_key in label_to_idx:\n",
    "            pi = label_to_idx[pose_key]\n",
    "            hi = label_to_idx[hand_key]\n",
    "            adj[pi, hi] = 1\n",
    "            adj[hi, pi] = 1\n",
    "\n",
    "    nose_key = ('pose', 'NOSE')\n",
    "    if nose_key in label_to_idx:\n",
    "        ni = label_to_idx[nose_key]\n",
    "        for fi, lbl in enumerate(node_labels):\n",
    "            if lbl[0] == 'face':\n",
    "                fi_idx = label_to_idx[lbl]\n",
    "                adj[ni, fi_idx] = 1\n",
    "                adj[fi_idx, ni] = 1\n",
    "\n",
    "    return adj\n",
    "\n",
    "def extract_compact_landmarks(video_path, holistic_model):\n",
    "    \"\"\"\n",
    "    Returns a tuple:\n",
    "    (array (T, num_nodes, 4) -> (x,y,z,conf) or None on failure,\n",
    "     array (T, 1) -> torso length or None,\n",
    "     array (T, num_bones, 3) -> bone vectors or None)\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Could not open\", video_path)\n",
    "        return None, None, None\n",
    "\n",
    "    frames = []\n",
    "    torso_lengths = []\n",
    "    bone_vectors_per_frame = []\n",
    "\n",
    "    try:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "            results = holistic_model.process(image)\n",
    "            image.flags.writeable = True\n",
    "\n",
    "            node_features = []\n",
    "            frame_torso_length = 0.0\n",
    "            frame_bone_vectors = []\n",
    "\n",
    "            # --- Extract Pose, Hands, Face Landmarks ---\n",
    "            if results.pose_landmarks:\n",
    "                for p in config[\"POSE_NODES\"]:\n",
    "                    lm = results.pose_landmarks.landmark[p]\n",
    "                    node_features.append(lm_to_xyzc(lm, default_conf=getattr(lm, 'visibility', 1.0)))\n",
    "                # midpoints\n",
    "                left_sh = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_SHOULDER]\n",
    "                right_sh = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_SHOULDER]\n",
    "                mid_sh = [(left_sh.x + right_sh.x)/2, (left_sh.y + right_sh.y)/2, (left_sh.z + right_sh.z)/2, 1.0]\n",
    "                node_features.append(mid_sh)\n",
    "\n",
    "                left_hip = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_HIP]\n",
    "                right_hip = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_HIP]\n",
    "                mid_hip = [(left_hip.x + right_hip.x)/2, (left_hip.y + right_hip.y)/2, (left_hip.z + right_hip.z)/2, 1.0]\n",
    "                node_features.append(mid_hip)\n",
    "                \n",
    "                # --- Calculate Torso Length (for normalization) ---\n",
    "                # We'll use the distance between the mid-shoulder and mid-hip points\n",
    "                # as a robust measure of torso length.\n",
    "                dx = mid_sh[0] - mid_hip[0]\n",
    "                dy = mid_sh[1] - mid_hip[1]\n",
    "                dz = mid_sh[2] - mid_hip[2]\n",
    "                frame_torso_length = sqrt(dx**2 + dy**2 + dz**2)\n",
    "            else:\n",
    "                for _ in range(len(config[\"POSE_NODES\"]) + 2):\n",
    "                    node_features.append([0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "            # Left hand\n",
    "            if results.left_hand_landmarks:\n",
    "                for lm in results.left_hand_landmarks.landmark:\n",
    "                    node_features.append(lm_to_xyzc(lm, default_conf=1.0))\n",
    "            else:\n",
    "                for _ in range(21):\n",
    "                    node_features.append([0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "            # Right hand\n",
    "            if results.right_hand_landmarks:\n",
    "                for lm in results.right_hand_landmarks.landmark:\n",
    "                    node_features.append(lm_to_xyzc(lm, default_conf=1.0))\n",
    "            else:\n",
    "                for _ in range(21):\n",
    "                    node_features.append([0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "            # Face (~8 points)\n",
    "            if results.face_landmarks:\n",
    "                for idx in FACE_INDICES_TO_KEEP:\n",
    "                    if idx < len(results.face_landmarks.landmark):\n",
    "                        lm = results.face_landmarks.landmark[idx]\n",
    "                        node_features.append(lm_to_xyzc(lm, default_conf=1.0))\n",
    "                    else:\n",
    "                        node_features.append([0.0, 0.0, 0.0, 0.0])\n",
    "            else:\n",
    "                for _ in range(len(FACE_INDICES_TO_KEEP)):\n",
    "                    node_features.append([0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "            # --- Calculate Bone Vectors ---\n",
    "            # Bone vectors represent the direction and length of a body segment.\n",
    "            # We'll calculate a few key vectors that capture significant movements.\n",
    "            # A vector from point A to B is B_coords - A_coords.\n",
    "            if results.pose_landmarks:\n",
    "                lm = results.pose_landmarks.landmark\n",
    "                # Right Arm\n",
    "                for start, end in [(mp_holistic.PoseLandmark.RIGHT_SHOULDER, mp_holistic.PoseLandmark.RIGHT_ELBOW),\n",
    "                                   (mp_holistic.PoseLandmark.RIGHT_ELBOW, mp_holistic.PoseLandmark.RIGHT_WRIST)]:\n",
    "                    vec = [lm[end].x - lm[start].x, lm[end].y - lm[start].y, lm[end].z - lm[start].z]\n",
    "                    frame_bone_vectors.extend(vec)\n",
    "                # Left Arm\n",
    "                for start, end in [(mp_holistic.PoseLandmark.LEFT_SHOULDER, mp_holistic.PoseLandmark.LEFT_ELBOW),\n",
    "                                   (mp_holistic.PoseLandmark.LEFT_ELBOW, mp_holistic.PoseLandmark.LEFT_WRIST)]:\n",
    "                    vec = [lm[end].x - lm[start].x, lm[end].y - lm[start].y, lm[end].z - lm[start].z]\n",
    "                    frame_bone_vectors.extend(vec)\n",
    "            else:\n",
    "                # Pad with zeros if landmarks are not detected\n",
    "                frame_bone_vectors = [0.0] * 12 # 4 bones * 3 axes\n",
    "            \n",
    "            frames.append(np.array(node_features, dtype=np.float32))\n",
    "            torso_lengths.append(frame_torso_length)\n",
    "            bone_vectors_per_frame.append(frame_bone_vectors)\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "\n",
    "    if not frames:\n",
    "        return None, None, None\n",
    "    \n",
    "    # Stack the arrays from the list of frames\n",
    "    landmark_seq = np.stack(frames, axis=0)\n",
    "    torso_seq = np.array(torso_lengths, dtype=np.float32)\n",
    "    bone_seq = np.array(bone_vectors_per_frame, dtype=np.float32)\n",
    "    \n",
    "    return landmark_seq, torso_seq, bone_seq\n",
    "\n",
    "def main():\n",
    "    with open(config[\"SPLIT_FILE_PATH\"], 'r') as f:\n",
    "        data = json.load(f)\n",
    "    video_ids = list(data.keys())\n",
    "    video_paths = [(vid, os.path.join(config[\"VIDEO_SOURCE_DIR\"], f\"{vid}.mp4\")) for vid in video_ids]\n",
    "\n",
    "    node_labels = []\n",
    "    for p in config[\"POSE_NODES\"]:\n",
    "        node_labels.append(('pose', p.name))\n",
    "    node_labels.append(('pose_mid', 'mid_shoulder'))\n",
    "    node_labels.append(('pose_mid', 'mid_hip'))\n",
    "    for i in range(21):\n",
    "        node_labels.append(('hand', f\"L_{i}\"))\n",
    "    for i in range(21):\n",
    "        node_labels.append(('hand', f\"R_{i}\"))\n",
    "    for idx in FACE_INDICES_TO_KEEP:\n",
    "        node_labels.append(('face', f\"F_{idx}\"))\n",
    "\n",
    "    adjacency = build_adjacency_map(node_labels)\n",
    "\n",
    "    holistic = mp_holistic.Holistic(static_image_mode=False,\n",
    "                                    model_complexity=1,\n",
    "                                    min_detection_confidence=0.5,\n",
    "                                    min_tracking_confidence=0.5)\n",
    "\n",
    "    extracted_landmarks = {}\n",
    "    extracted_torsos = {}\n",
    "    extracted_bones = {}\n",
    "\n",
    "    try:\n",
    "        for i, (vid, path) in enumerate(tqdm(video_paths, desc=\"Videos\")):\n",
    "            if not os.path.exists(path):\n",
    "                continue\n",
    "            seq, torso_len, bone_vec = extract_compact_landmarks(path, holistic)\n",
    "            if seq is not None:\n",
    "                extracted_landmarks[vid] = seq\n",
    "                extracted_torsos[vid] = torso_len\n",
    "                extracted_bones[vid] = bone_vec\n",
    "            if (i > 0) and (i+1) % config[\"SAVE_CHECKPOINT_EVERY_N_VIDEOS\"] == 0:\n",
    "                print(f\"Saving checkpoint with {len(extracted_landmarks)} videos\")\n",
    "                np.savez_compressed(config[\"OUTPUT_NPZ_PATH\"],\n",
    "                                     _node_labels=np.array(node_labels, dtype=object),\n",
    "                                     _adjacency=adjacency,\n",
    "                                     _torso_lengths=extracted_torsos,\n",
    "                                     _bone_vectors=extracted_bones,\n",
    "                                     **extracted_landmarks)\n",
    "    finally:\n",
    "        holistic.close()\n",
    "\n",
    "    np.savez_compressed(config[\"OUTPUT_NPZ_PATH\"],\n",
    "                        _node_labels=np.array(node_labels, dtype=object),\n",
    "                        _adjacency=adjacency,\n",
    "                        _torso_lengths=extracted_torsos,\n",
    "                        _bone_vectors=extracted_bones,\n",
    "                        **extracted_landmarks)\n",
    "    print(\"Saved:\", config[\"OUTPUT_NPZ_PATH\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0b1be1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
