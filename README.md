ğŸ§  Sign Language Recognition using ST-GCN and Pose Landmarks

Deep learning-based Sign Language Recognition (SLR) pipeline built using PyTorch and Graph Convolutional Networks (ST-GCN) on human pose landmarks extracted from sign videos.

ğŸ“ Project Overview

This repository implements a Spatial-Temporal Graph Convolutional Network (ST-GCN) model trained on pose keypoints (hands, face, body) extracted from sign language videos using MediaPipe.
It supports both 100-class and 300-class configurations from the WLASL dataset.

The workflow includes:

Landmark Extraction from videos

Graph-based Spatio-Temporal Modeling

Training & Evaluation with augmentation and attention modules

âš™ï¸ Project Structure

ğŸ“¦ Spatial-Temporal Graph Convolutional Network (ST-GCN)-Using-Pytorch-and-Tensor-Flow
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract_landmarks.ipynb         # Extracts pose/hand/face landmarks using MediaPipe
â”‚   â”œâ”€â”€ train_stgcn_100.ipynb           # ST-GCN training for 100 sign classes
â”‚   â””â”€â”€ train_stgcn_300.ipynb           # ST-GCN training for 300 sign classes
â”‚
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”œâ”€â”€ .gitignore                          # Ignore unnecessary files
â””â”€â”€ README.md                           

ğŸ§© Features

âœ… Pose-based sign representation (no raw video needed)
âœ… Graph Convolutional Network (ST-GCN) backbone
âœ… Temporal self-attention for motion emphasis
âœ… Mixup and label smoothing for regularization
âœ… Support for 100-class and 300-class subsets
âœ… AMP training for faster GPU computation
âœ… Automatic best model checkpoint saving

ğŸ§® Workflow
1ï¸âƒ£ Extract Landmarks
src/extract_landmarks.ipynb

2ï¸âƒ£ Train ST-GCN
src/train_stgcn_100.ipynb # for 100 classes
src/train_stgcn_300.ipynb # for 300 classes

ğŸ§  Model Architecture
ST-GCN Block

Each block learns:

Spatial Features: joint-to-joint relations via graph convolution

Temporal Features: motion over time via temporal convolution

Temporal Self-Attention

Focuses on important motion frames, enhancing gesture clarity.

Input (B, 4, 128, 67)
   â†“
ST-GCN Ã— 4
   â†“
Temporal Attention
   â†“
Global Pooling â†’ Linear(256 â†’ Classes)

ğŸ§‘â€ğŸ’» Author

Muhammad Huzaifa

Deep Learning Researcher | Focus: Sign Language Recognition, Transformers, AGI
ğŸ“§ Contact: mhuzaifa3202@gmail.com





